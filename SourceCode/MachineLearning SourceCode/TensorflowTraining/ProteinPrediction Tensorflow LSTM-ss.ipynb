{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edward\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Edward\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Edward\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Edward\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Edward\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Edward\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Edward\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Edward\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Edward\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Edward\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Edward\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Edward\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete without error\n"
     ]
    }
   ],
   "source": [
    "##Author: Wong Yuk Ming, Edward\n",
    "##Remarks: it seems using keras does not require us to convert numpy array to tensor, it will done once we fit the data\n",
    "#References:\n",
    "#https://www.dlology.com/blog/how-to-choose-last-layer-activation-and-loss-function/\n",
    "import os \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy import load\n",
    "\n",
    "#loading the file from desire directory\n",
    "path = \"../itasserSx9\" ##<-modify this directory is ok\n",
    "\n",
    "filenames= os.listdir(path)\n",
    "x_List=[]\n",
    "y_List=[]\n",
    "xyfileTuple=[]\n",
    "\n",
    "#since on linux file are not ordered by name, there is no way to guarentee n+1 file is the y file of x file n\n",
    "#there will not be any selection that limit the number of file to process\n",
    "\n",
    "for names in filenames:\n",
    "    if names.endswith(\"-x-.npy\"):\n",
    "        x_List.append(names)\n",
    "    else:\n",
    "        y_List.append(names)\n",
    "\n",
    "##testing purpose\n",
    "#print(x_List)\n",
    "#print(y_List)\n",
    "\n",
    "for xfile in x_List:\n",
    "    for yfile in y_List:\n",
    "        if (xfile.replace('-x-.npy','') == yfile.replace('-y-.npy','')):\n",
    "            xyfileTuple.append((xfile,yfile))\n",
    "            break\n",
    "##testing purpose\n",
    "#print(xyfileTuple)\n",
    "\n",
    "#notice that this cell has no error\n",
    "print('complete without error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "complete without error\n"
     ]
    }
   ],
   "source": [
    "#from filename convert to numpy array \n",
    "##input:  array of tuple contains x file name and y file name, purpose <\"angle\", \"nonangle\">\n",
    "##output: array of tuple contains x numpy array and y numpy array\n",
    "def extractData(tupleToConv, purpose):\n",
    "    outputTensorTuple=[]\n",
    "    i=0\n",
    "    for pair in tupleToConv:\n",
    "        if(purpose=='angle'):\n",
    "            xdatanp=load(path+\"/\"+pair[0])\n",
    "            xdatatf=tf.convert_to_tensor(xdatanp, dtype=np.float32)\n",
    "            \n",
    "            ydatanp=load(path+\"/\"+pair[1])\n",
    "            #print(ydatanp)\n",
    "            ydatanp=ydatanp[:,4:6]\n",
    "            #if(i==0):\n",
    "                #print('from y')\n",
    "                #print(ydatanp)\n",
    "            #ydatatf=tf.convert_to_tensor(xdatanp, dtype=np.float32)\n",
    "            outputTensorTuple.append((xdatanp,ydatanp))\n",
    "            \n",
    "        else:\n",
    "            xdatanp=load(path+\"/\"+pair[0])\n",
    "            xdatatf=tf.convert_to_tensor(xdatanp, dtype=np.float32)\n",
    "            \n",
    "            ydatanp=load(path+\"/\"+pair[1])\n",
    "            ydatanp=ydatanp[:,:4]\n",
    "            if(i==0):\n",
    "                print(ydatanp)\n",
    "            #ydatatf=tf.convert_to_tensor(xdatanp, dtype=np.float32)\n",
    "            outputTensorTuple.append((xdatanp,ydatanp))\n",
    "        \n",
    "        i=i+1\n",
    "            \n",
    "    return outputTensorTuple\n",
    "\n",
    "angleAllData = extractData(xyfileTuple,purpose='angle')\n",
    "ssAllData = extractData(xyfileTuple,purpose='nonangle')\n",
    "\n",
    "#notice that this cell has no error\n",
    "print('complete without error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete without error\n"
     ]
    }
   ],
   "source": [
    "#extract to k-mers where k=18 for each amino acid position\n",
    "#for amino acid at position n, x= n-18:n, y=ny\n",
    "#also add padding\n",
    "\n",
    "\n",
    "angleKmersData=[]\n",
    "ssKmersData=[]\n",
    "\n",
    "#specify how many file want to process in here\n",
    "numFileProcess=100 ##<-modify this for changing -1 = process all file\n",
    "if (numFileProcess==-1):\n",
    "    numFilProcess=len(angleAllData)\n",
    "\n",
    "for i in range(numFileProcess):\n",
    "    thisAngleTuple = angleAllData[i]\n",
    "    thisSSTuple = ssAllData[i]\n",
    "    ##reporter\n",
    "    #if(i==0):\n",
    "        #print(thisAngleTuple[0].shape)\n",
    "        #print(thisAngleTuple[0][0:18,:])\n",
    "    for j in range(thisAngleTuple[0].shape[0]):\n",
    "        x_vec=None\n",
    "        y_vec=None\n",
    "        if(j<18):\n",
    "            #print(j)\n",
    "            x_vec=thisAngleTuple[0][0:j+1,:]\n",
    "            x_vec=np.insert(x_vec, 0 , np.zeros([17-j,9]) , axis=0)#adding padding\n",
    "            #print(x_vec)\n",
    "            #print(x_vec.shape)\n",
    "        else:\n",
    "            #print(j)\n",
    "            x_vec=thisAngleTuple[0][j-17:j+1,:]\n",
    "            #if(j==18):\n",
    "            #    print(x_vec)\n",
    "            #print(x_vec.shape)\n",
    "        y_vec=thisAngleTuple[1][j,:]\n",
    "        #x_vec=tf.convert_to_tensor(x_vec, dtype=np.float32)\n",
    "        #y_vec=tf.convert_to_tensor(y_vec, dtype=np.float32)\n",
    "        angleKmersData.append((x_vec,y_vec))\n",
    "    for j in range(thisAngleTuple[0].shape[0]):\n",
    "        x_vec=None\n",
    "        y_vec=None\n",
    "        if(j<18):\n",
    "            #print(j)\n",
    "            x_vec=thisSSTuple[0][0:j+1,:]\n",
    "            x_vec=np.insert(x_vec, 0 , np.zeros([17-j,9]) , axis=0)#adding padding\n",
    "            #print(x_vec)\n",
    "            #print(x_vec.shape)\n",
    "        else:\n",
    "            #print(j)\n",
    "            x_vec=thisSSTuple[0][j-17:j+1,:]\n",
    "            #if(j==18):\n",
    "            #    print(x_vec)\n",
    "            #print(x_vec.shape)\n",
    "        y_vec=thisSSTuple[1][j,:]\n",
    "        #x_vec=tf.convert_to_tensor(x_vec, dtype=np.float32)\n",
    "        #y_vec=tf.convert_to_tensor(y_vec, dtype=np.float32)\n",
    "        ssKmersData.append((x_vec,y_vec))\n",
    "\n",
    "'''\n",
    "for tup in angleKmersData:\n",
    "    print(str(tup[0].shape)+\" = \"+str(tup[1].shape))\n",
    "for tup in ssKmersData:\n",
    "    print(str(tup[0].shape)+\" = \"+str(tup[1].shape))\n",
    "'''\n",
    "#notice that this cell has no error\n",
    "print('complete without error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete without error\n"
     ]
    }
   ],
   "source": [
    "#for feeding into the model, it has to be converted from list of 2D array to a 3D array\n",
    "#for angle\n",
    "from math import floor\n",
    "from random import random\n",
    "Xa=list()\n",
    "Ya=list()\n",
    "\n",
    "Xatest=list()\n",
    "Yatest=list()\n",
    "#for secondary structure\n",
    "Xs=list()\n",
    "Ys=list()\n",
    "\n",
    "Xstest=list()\n",
    "Ystest=list()\n",
    "for item in angleKmersData:\n",
    "    splitting=floor(random()*10)\n",
    "    if(splitting>7):\n",
    "        Xatest.append(item[0])\n",
    "        Yatest.append(item[1])\n",
    "    else:\n",
    "        Xa.append(item[0])\n",
    "        Ya.append(item[1])\n",
    "for item in ssKmersData:\n",
    "    splitting=floor(random()*10)\n",
    "    if(splitting>7):\n",
    "        Xstest.append(item[0])\n",
    "        Ystest.append(item[1])\n",
    "    else:\n",
    "        Xs.append(item[0])\n",
    "        Ys.append(item[1])\n",
    "\n",
    "# checking\n",
    "#for i in range(len(Xa)):\n",
    "#\tprint(Xa[i], Ya[i])\n",
    "Xa=np.array(Xa) #[samples=S timestamp=18 features=9]\n",
    "Ya=np.array(Ya) #[sample=S features=2]\n",
    "Xs=np.array(Xs)\n",
    "Ys=np.array(Ys)\n",
    "\n",
    "Xatest=np.array(Xatest) #[samples=S timestamp=18 features=9]\n",
    "Yatest=np.array(Yatest) #[sample=S features=2]\n",
    "Xstest=np.array(Xstest)\n",
    "Ystest=np.array(Ystest)\n",
    "\n",
    "#print(Xa.shape)\n",
    "#Xa=Xa.reshape((Xa.shape[0],Xa.shape[1],9))\n",
    "#print(Ya.shape)\n",
    "#print(Xa.shape)\n",
    "\n",
    "#notice that this cell has no error\n",
    "print('complete without error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete without error\n"
     ]
    }
   ],
   "source": [
    "#import all things needed for tensorflow LSTM network\n",
    "#this is for predicting the next number\n",
    "from tensorflow.keras.models import Sequential #Sequential model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Flatten #CNN\n",
    "from tensorflow.keras.layers import TimeDistributed #CNN\n",
    "from tensorflow.keras.layers import Conv1D       #CNN\n",
    "from tensorflow.keras.layers import MaxPooling1D #CNN\n",
    "\n",
    "#notice that this cell has no error\n",
    "print('complete without error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Edward\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Edward\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Edward\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Edward\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 42060 samples, validate on 10534 samples\n",
      "Epoch 1/100\n",
      "  500/42060 [..............................] - ETA: 6:00 - loss: 1.4708 - acc: 0.1580WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.062657). Check your callbacks.\n",
      "42060/42060 [==============================] - 22s 523us/sample - loss: 1.3218 - acc: 0.3963 - val_loss: 1.2938 - val_acc: 0.4143\n",
      "Epoch 2/100\n",
      "42060/42060 [==============================] - 17s 393us/sample - loss: 1.2713 - acc: 0.4247 - val_loss: 1.2843 - val_acc: 0.4144\n",
      "Epoch 3/100\n",
      "42060/42060 [==============================] - 17s 400us/sample - loss: 1.2566 - acc: 0.4353 - val_loss: 1.2726 - val_acc: 0.4194\n",
      "Epoch 4/100\n",
      "42060/42060 [==============================] - 16s 390us/sample - loss: 1.2433 - acc: 0.4437 - val_loss: 1.2626 - val_acc: 0.4406\n",
      "Epoch 5/100\n",
      "42060/42060 [==============================] - 17s 397us/sample - loss: 1.2325 - acc: 0.4533 - val_loss: 1.2543 - val_acc: 0.4362\n",
      "Epoch 6/100\n",
      "42060/42060 [==============================] - 16s 386us/sample - loss: 1.2228 - acc: 0.4570 - val_loss: 1.3064 - val_acc: 0.4256\n",
      "Epoch 7/100\n",
      "42060/42060 [==============================] - 16s 373us/sample - loss: 1.2122 - acc: 0.4650 - val_loss: 1.2871 - val_acc: 0.4217\n",
      "Epoch 8/100\n",
      "42060/42060 [==============================] - 16s 381us/sample - loss: 1.2016 - acc: 0.4709 - val_loss: 1.2582 - val_acc: 0.4591\n",
      "Epoch 9/100\n",
      "42060/42060 [==============================] - 17s 393us/sample - loss: 1.1873 - acc: 0.4762 - val_loss: 1.2890 - val_acc: 0.4490\n",
      "Epoch 10/100\n",
      "42060/42060 [==============================] - 16s 385us/sample - loss: 1.1685 - acc: 0.4902 - val_loss: 1.3036 - val_acc: 0.4354\n",
      "Epoch 11/100\n",
      "42060/42060 [==============================] - 17s 404us/sample - loss: 1.1498 - acc: 0.5008 - val_loss: 1.2364 - val_acc: 0.4693\n",
      "Epoch 12/100\n",
      "42060/42060 [==============================] - 17s 411us/sample - loss: 1.1225 - acc: 0.5168 - val_loss: 1.1948 - val_acc: 0.4851\n",
      "Epoch 13/100\n",
      "42060/42060 [==============================] - 17s 399us/sample - loss: 1.0912 - acc: 0.5351 - val_loss: 1.1825 - val_acc: 0.5006\n",
      "Epoch 14/100\n",
      "42060/42060 [==============================] - 17s 402us/sample - loss: 1.0543 - acc: 0.5530 - val_loss: 1.2432 - val_acc: 0.4848\n",
      "Epoch 15/100\n",
      "42060/42060 [==============================] - 16s 389us/sample - loss: 1.0117 - acc: 0.5749 - val_loss: 1.2012 - val_acc: 0.5011\n",
      "Epoch 16/100\n",
      "42060/42060 [==============================] - 16s 387us/sample - loss: 0.9675 - acc: 0.5948 - val_loss: 1.1924 - val_acc: 0.4945\n",
      "Epoch 17/100\n",
      "42060/42060 [==============================] - 16s 389us/sample - loss: 0.9151 - acc: 0.6230 - val_loss: 1.1584 - val_acc: 0.5184\n",
      "Epoch 18/100\n",
      "42060/42060 [==============================] - 17s 403us/sample - loss: 0.8619 - acc: 0.6456 - val_loss: 1.1455 - val_acc: 0.5253\n",
      "Epoch 19/100\n",
      "42060/42060 [==============================] - 16s 383us/sample - loss: 0.8093 - acc: 0.6701 - val_loss: 1.1354 - val_acc: 0.5246\n",
      "Epoch 20/100\n",
      "42060/42060 [==============================] - 17s 398us/sample - loss: 0.7605 - acc: 0.6930 - val_loss: 1.1923 - val_acc: 0.5383\n",
      "Epoch 21/100\n",
      "42060/42060 [==============================] - 16s 392us/sample - loss: 0.7099 - acc: 0.7158 - val_loss: 1.0720 - val_acc: 0.5767\n",
      "Epoch 22/100\n",
      "42060/42060 [==============================] - 17s 403us/sample - loss: 0.6578 - acc: 0.7408 - val_loss: 1.1313 - val_acc: 0.5659\n",
      "Epoch 23/100\n",
      "42060/42060 [==============================] - 17s 399us/sample - loss: 0.6130 - acc: 0.7572 - val_loss: 1.1254 - val_acc: 0.5623\n",
      "Epoch 24/100\n",
      "42060/42060 [==============================] - 17s 396us/sample - loss: 0.5697 - acc: 0.7741 - val_loss: 1.0875 - val_acc: 0.5818\n",
      "Epoch 25/100\n",
      "42060/42060 [==============================] - 16s 391us/sample - loss: 0.5261 - acc: 0.7946 - val_loss: 1.0425 - val_acc: 0.6004\n",
      "Epoch 26/100\n",
      "42060/42060 [==============================] - 16s 391us/sample - loss: 0.4821 - acc: 0.8116 - val_loss: 1.1135 - val_acc: 0.5918\n",
      "Epoch 27/100\n",
      "42060/42060 [==============================] - 16s 386us/sample - loss: 0.4482 - acc: 0.8259 - val_loss: 1.1147 - val_acc: 0.5989\n",
      "Epoch 28/100\n",
      "42060/42060 [==============================] - 17s 404us/sample - loss: 0.4119 - acc: 0.8435 - val_loss: 1.1549 - val_acc: 0.5990\n",
      "Epoch 29/100\n",
      "42060/42060 [==============================] - 17s 415us/sample - loss: 0.3739 - acc: 0.8578 - val_loss: 1.3196 - val_acc: 0.5796\n",
      "Epoch 30/100\n",
      "42060/42060 [==============================] - 17s 402us/sample - loss: 0.3489 - acc: 0.8687 - val_loss: 1.0708 - val_acc: 0.6284\n",
      "Epoch 31/100\n",
      "42060/42060 [==============================] - 17s 406us/sample - loss: 0.3143 - acc: 0.8816 - val_loss: 1.2818 - val_acc: 0.5937\n",
      "Epoch 32/100\n",
      "42060/42060 [==============================] - 17s 397us/sample - loss: 0.2875 - acc: 0.8960 - val_loss: 1.2125 - val_acc: 0.6130\n",
      "Epoch 33/100\n",
      "42060/42060 [==============================] - 16s 392us/sample - loss: 0.2644 - acc: 0.9053 - val_loss: 1.4021 - val_acc: 0.5823\n",
      "Epoch 34/100\n",
      "42060/42060 [==============================] - 16s 382us/sample - loss: 0.2418 - acc: 0.9163 - val_loss: 1.2845 - val_acc: 0.6160\n",
      "Epoch 35/100\n",
      "42060/42060 [==============================] - 17s 403us/sample - loss: 0.2172 - acc: 0.9247 - val_loss: 1.2592 - val_acc: 0.6283\n",
      "Epoch 36/100\n",
      "42060/42060 [==============================] - 16s 388us/sample - loss: 0.1965 - acc: 0.9330 - val_loss: 1.3987 - val_acc: 0.5979\n",
      "Epoch 37/100\n",
      "42060/42060 [==============================] - 16s 382us/sample - loss: 0.1819 - acc: 0.9387 - val_loss: 1.3293 - val_acc: 0.6304\n",
      "Epoch 38/100\n",
      "42060/42060 [==============================] - 17s 400us/sample - loss: 0.1642 - acc: 0.9477 - val_loss: 1.3211 - val_acc: 0.6402\n",
      "Epoch 39/100\n",
      "42060/42060 [==============================] - 16s 389us/sample - loss: 0.1515 - acc: 0.9515 - val_loss: 1.3526 - val_acc: 0.6453\n",
      "Epoch 40/100\n",
      "42060/42060 [==============================] - 17s 402us/sample - loss: 0.1365 - acc: 0.9571 - val_loss: 1.4999 - val_acc: 0.6147\n",
      "Epoch 41/100\n",
      "42060/42060 [==============================] - 17s 393us/sample - loss: 0.1256 - acc: 0.9614 - val_loss: 1.4457 - val_acc: 0.6358\n",
      "Epoch 42/100\n",
      "42060/42060 [==============================] - 17s 412us/sample - loss: 0.1147 - acc: 0.9666 - val_loss: 1.8753 - val_acc: 0.5882\n",
      "Epoch 43/100\n",
      "42060/42060 [==============================] - 17s 397us/sample - loss: 0.1130 - acc: 0.9664 - val_loss: 1.5615 - val_acc: 0.6191\n",
      "Epoch 44/100\n",
      "42060/42060 [==============================] - 17s 395us/sample - loss: 0.0961 - acc: 0.9718 - val_loss: 1.5628 - val_acc: 0.6352\n",
      "Epoch 45/100\n",
      "42060/42060 [==============================] - 16s 392us/sample - loss: 0.0901 - acc: 0.9738 - val_loss: 1.4697 - val_acc: 0.6432\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42060/42060 [==============================] - 17s 401us/sample - loss: 0.0844 - acc: 0.9758 - val_loss: 1.5226 - val_acc: 0.6579\n",
      "Epoch 47/100\n",
      "42060/42060 [==============================] - 16s 387us/sample - loss: 0.0809 - acc: 0.9750 - val_loss: 1.5894 - val_acc: 0.6426\n",
      "Epoch 48/100\n",
      "42060/42060 [==============================] - 16s 388us/sample - loss: 0.0743 - acc: 0.9791 - val_loss: 1.6296 - val_acc: 0.6528\n",
      "Epoch 49/100\n",
      "42060/42060 [==============================] - 17s 398us/sample - loss: 0.0703 - acc: 0.9800 - val_loss: 1.6923 - val_acc: 0.6204\n",
      "Epoch 50/100\n",
      "42060/42060 [==============================] - 17s 393us/sample - loss: 0.0669 - acc: 0.9809 - val_loss: 1.6849 - val_acc: 0.6482\n",
      "Epoch 51/100\n",
      "42060/42060 [==============================] - 16s 386us/sample - loss: 0.0621 - acc: 0.9823 - val_loss: 1.6914 - val_acc: 0.6525\n",
      "Epoch 52/100\n",
      "42060/42060 [==============================] - 17s 393us/sample - loss: 0.0618 - acc: 0.9827 - val_loss: 1.7734 - val_acc: 0.6305\n",
      "Epoch 53/100\n",
      "42060/42060 [==============================] - 16s 381us/sample - loss: 0.0632 - acc: 0.9820 - val_loss: 1.7404 - val_acc: 0.6428\n",
      "Epoch 54/100\n",
      "42060/42060 [==============================] - 16s 386us/sample - loss: 0.0553 - acc: 0.9850 - val_loss: 1.7946 - val_acc: 0.6260\n",
      "Epoch 55/100\n",
      "42060/42060 [==============================] - 16s 391us/sample - loss: 0.0569 - acc: 0.9837 - val_loss: 1.8880 - val_acc: 0.6266\n",
      "Epoch 56/100\n",
      "42060/42060 [==============================] - 17s 405us/sample - loss: 0.0511 - acc: 0.9856 - val_loss: 1.7090 - val_acc: 0.6511\n",
      "Epoch 57/100\n",
      "42060/42060 [==============================] - 17s 398us/sample - loss: 0.0524 - acc: 0.9852 - val_loss: 1.8082 - val_acc: 0.6397\n",
      "Epoch 58/100\n",
      "42060/42060 [==============================] - 17s 394us/sample - loss: 0.0469 - acc: 0.9865 - val_loss: 1.7911 - val_acc: 0.6541\n",
      "Epoch 59/100\n",
      "42060/42060 [==============================] - 16s 392us/sample - loss: 0.0463 - acc: 0.9865 - val_loss: 1.8230 - val_acc: 0.6469\n",
      "Epoch 60/100\n",
      "42060/42060 [==============================] - 17s 399us/sample - loss: 0.0459 - acc: 0.9871 - val_loss: 1.7770 - val_acc: 0.6573\n",
      "Epoch 61/100\n",
      "42060/42060 [==============================] - 17s 404us/sample - loss: 0.0433 - acc: 0.9879 - val_loss: 1.8578 - val_acc: 0.6428\n",
      "Epoch 62/100\n",
      "42060/42060 [==============================] - 17s 393us/sample - loss: 0.0443 - acc: 0.9874 - val_loss: 1.7794 - val_acc: 0.6541\n",
      "Epoch 63/100\n",
      "42060/42060 [==============================] - 17s 404us/sample - loss: 0.0399 - acc: 0.9891 - val_loss: 1.8843 - val_acc: 0.6316\n",
      "Epoch 64/100\n",
      "42060/42060 [==============================] - 16s 381us/sample - loss: 0.0404 - acc: 0.9887 - val_loss: 1.8623 - val_acc: 0.6484\n",
      "Epoch 65/100\n",
      "42060/42060 [==============================] - 16s 381us/sample - loss: 0.0420 - acc: 0.9870 - val_loss: 1.9343 - val_acc: 0.6457\n",
      "Epoch 66/100\n",
      "42060/42060 [==============================] - 17s 397us/sample - loss: 0.0400 - acc: 0.9883 - val_loss: 1.8319 - val_acc: 0.6618\n",
      "Epoch 67/100\n",
      "42060/42060 [==============================] - 17s 395us/sample - loss: 0.0408 - acc: 0.9882 - val_loss: 1.9487 - val_acc: 0.6490\n",
      "Epoch 68/100\n",
      "42060/42060 [==============================] - 17s 398us/sample - loss: 0.0365 - acc: 0.9898 - val_loss: 1.9790 - val_acc: 0.6445\n",
      "Epoch 69/100\n",
      "42060/42060 [==============================] - 17s 403us/sample - loss: 0.0343 - acc: 0.9900 - val_loss: 2.0532 - val_acc: 0.6308\n",
      "Epoch 70/100\n",
      "42060/42060 [==============================] - 16s 391us/sample - loss: 0.0381 - acc: 0.9884 - val_loss: 1.9785 - val_acc: 0.6457\n",
      "Epoch 71/100\n",
      "42060/42060 [==============================] - 17s 400us/sample - loss: 0.0354 - acc: 0.9895 - val_loss: 1.9249 - val_acc: 0.6543\n",
      "Epoch 72/100\n",
      "42060/42060 [==============================] - 16s 391us/sample - loss: 0.0352 - acc: 0.9898 - val_loss: 2.0491 - val_acc: 0.6293\n",
      "Epoch 73/100\n",
      "42060/42060 [==============================] - 16s 391us/sample - loss: 0.0329 - acc: 0.9904 - val_loss: 1.9274 - val_acc: 0.6598\n",
      "Epoch 74/100\n",
      "42060/42060 [==============================] - 17s 395us/sample - loss: 0.0305 - acc: 0.9912 - val_loss: 2.0146 - val_acc: 0.6400\n",
      "Epoch 75/100\n",
      "42060/42060 [==============================] - 17s 408us/sample - loss: 0.0331 - acc: 0.9905 - val_loss: 2.0126 - val_acc: 0.6473\n",
      "Epoch 76/100\n",
      "42060/42060 [==============================] - 17s 407us/sample - loss: 0.0298 - acc: 0.9915 - val_loss: 1.9561 - val_acc: 0.6586\n",
      "Epoch 77/100\n",
      "42060/42060 [==============================] - 17s 403us/sample - loss: 0.0289 - acc: 0.9918 - val_loss: 2.0122 - val_acc: 0.6561\n",
      "Epoch 78/100\n",
      "42060/42060 [==============================] - 17s 394us/sample - loss: 0.0337 - acc: 0.9906 - val_loss: 2.1306 - val_acc: 0.6348\n",
      "Epoch 79/100\n",
      "42060/42060 [==============================] - 17s 396us/sample - loss: 0.0300 - acc: 0.9912 - val_loss: 2.2133 - val_acc: 0.6306\n",
      "Epoch 80/100\n",
      "42060/42060 [==============================] - 17s 397us/sample - loss: 0.0295 - acc: 0.9917 - val_loss: 2.0053 - val_acc: 0.6567\n",
      "Epoch 81/100\n",
      "42060/42060 [==============================] - 17s 403us/sample - loss: 0.0297 - acc: 0.9913 - val_loss: 2.1042 - val_acc: 0.6438\n",
      "Epoch 82/100\n",
      "42060/42060 [==============================] - 16s 387us/sample - loss: 0.0270 - acc: 0.9924 - val_loss: 2.1602 - val_acc: 0.6395\n",
      "Epoch 83/100\n",
      "42060/42060 [==============================] - 17s 405us/sample - loss: 0.0307 - acc: 0.9910 - val_loss: 1.9830 - val_acc: 0.6639\n",
      "Epoch 84/100\n",
      "42060/42060 [==============================] - 17s 396us/sample - loss: 0.0272 - acc: 0.9919 - val_loss: 2.2033 - val_acc: 0.6374\n",
      "Epoch 85/100\n",
      "42060/42060 [==============================] - 17s 404us/sample - loss: 0.0301 - acc: 0.9908 - val_loss: 2.1813 - val_acc: 0.6434\n",
      "Epoch 86/100\n",
      "42060/42060 [==============================] - 16s 391us/sample - loss: 0.0242 - acc: 0.9930 - val_loss: 2.0584 - val_acc: 0.6559\n",
      "Epoch 87/100\n",
      "42060/42060 [==============================] - 17s 415us/sample - loss: 0.0269 - acc: 0.9923 - val_loss: 2.2071 - val_acc: 0.6478\n",
      "Epoch 88/100\n",
      "42060/42060 [==============================] - 17s 407us/sample - loss: 0.0284 - acc: 0.9917 - val_loss: 2.2063 - val_acc: 0.6457\n",
      "Epoch 89/100\n",
      "42060/42060 [==============================] - 17s 407us/sample - loss: 0.0285 - acc: 0.9921 - val_loss: 2.2901 - val_acc: 0.6230\n",
      "Epoch 90/100\n",
      "42060/42060 [==============================] - 17s 409us/sample - loss: 0.0268 - acc: 0.9923 - val_loss: 2.1462 - val_acc: 0.6529\n",
      "Epoch 91/100\n",
      "42060/42060 [==============================] - 16s 390us/sample - loss: 0.0255 - acc: 0.9927 - val_loss: 2.1181 - val_acc: 0.6576\n",
      "Epoch 92/100\n",
      "42060/42060 [==============================] - 17s 394us/sample - loss: 0.0253 - acc: 0.9923 - val_loss: 2.1171 - val_acc: 0.6556\n",
      "Epoch 93/100\n",
      "42060/42060 [==============================] - 17s 396us/sample - loss: 0.0260 - acc: 0.9919 - val_loss: 2.3324 - val_acc: 0.6243\n",
      "Epoch 94/100\n",
      "42060/42060 [==============================] - 17s 397us/sample - loss: 0.0249 - acc: 0.9929 - val_loss: 2.0756 - val_acc: 0.6623\n",
      "Epoch 95/100\n",
      "42060/42060 [==============================] - 17s 407us/sample - loss: 0.0253 - acc: 0.9923 - val_loss: 2.1842 - val_acc: 0.6545\n",
      "Epoch 96/100\n",
      "42060/42060 [==============================] - 17s 398us/sample - loss: 0.0213 - acc: 0.9939 - val_loss: 2.1504 - val_acc: 0.6529\n",
      "Epoch 97/100\n",
      "42060/42060 [==============================] - 17s 398us/sample - loss: 0.0212 - acc: 0.9939 - val_loss: 2.2686 - val_acc: 0.6391\n",
      "Epoch 98/100\n",
      "42060/42060 [==============================] - 17s 398us/sample - loss: 0.0237 - acc: 0.9926 - val_loss: 2.1374 - val_acc: 0.6565\n",
      "Epoch 99/100\n",
      "42060/42060 [==============================] - 17s 400us/sample - loss: 0.0229 - acc: 0.9934 - val_loss: 2.4776 - val_acc: 0.6269\n",
      "Epoch 100/100\n",
      "42060/42060 [==============================] - 17s 403us/sample - loss: 0.0242 - acc: 0.9930 - val_loss: 2.2120 - val_acc: 0.6621\n",
      "complete without error\n"
     ]
    }
   ],
   "source": [
    "##setting up callback for tensor board\n",
    "# Load the TensorBoard notebook extension\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "\n",
    "import datetime\n",
    "#log_dir = \"./logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "##saving checkpoint\n",
    "model_checkpoint_callback1 = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='./saved_model/checkpoint/max',\n",
    "    save_weights_only=False,\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_best_only=True\n",
    "    \n",
    ")\n",
    "model_checkpoint_callback2 = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='./saved_model/checkpoint/e50',\n",
    "    save_weights_only=False,\n",
    "    save_freq=50,\n",
    "    save_best_only=False,\n",
    ")\n",
    "\n",
    "##setting up model\n",
    "\n",
    "# for CNN-LSTM reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "# if not using CNN, just comment this line\n",
    "#Xa=Xa.reshape(Xa.shape[0],Xa.shape[1],Xa.shape[2],1)\n",
    "\n",
    "#model\n",
    "model = Sequential()\n",
    "#model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, 9,1)))\n",
    "#model.add(TimeDistributed(MaxPooling1D(pool_size=9)))\n",
    "#model.add(TimeDistributed(Flatten()))\n",
    "model.add(Bidirectional(LSTM(50, activation='tanh',return_sequences=True, recurrent_activation='sigmoid', use_bias=True , input_shape=(18,9))))\n",
    "model.add(Bidirectional(LSTM(100, activation='relu',return_sequences=True, recurrent_activation='sigmoid', use_bias=True )))\n",
    "model.add(Bidirectional(LSTM(100, activation='tanh',return_sequences=True, recurrent_activation='sigmoid', use_bias=True )))\n",
    "model.add(Bidirectional(LSTM(100, activation='relu',return_sequences=True, recurrent_activation='sigmoid', use_bias=True )))\n",
    "model.add(Bidirectional(LSTM(100, activation='tanh',return_sequences=True, recurrent_activation='sigmoid', use_bias=True )))\n",
    "model.add(Bidirectional(LSTM(100, activation='relu',return_sequences=True, recurrent_activation='sigmoid', use_bias=True )))\n",
    "model.add(Bidirectional(LSTM(100, activation='tanh',return_sequences=True, recurrent_activation='sigmoid', use_bias=True )))\n",
    "model.add(Bidirectional(LSTM(50, activation='tanh',  recurrent_activation='sigmoid', use_bias=True )))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#fitting the model\n",
    "model.fit(x=Xs,\n",
    "          y=Ys,\n",
    "          batch_size=500,\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          callbacks=[#tensorboard_callback,\n",
    "              model_checkpoint_callback1,\n",
    "              model_checkpoint_callback2 ],\n",
    "          validation_data=(Xstest,Ystest)\n",
    "         )\n",
    "#starting tensorboard\n",
    "\n",
    "model.save('./saved_model/ssModel')\n",
    "\n",
    "#notice that this cell has no error\n",
    "print('complete without error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 449ms/sample\n",
      "[[1.6600883e-06 5.2202613e-06 1.6547819e-10 9.9999309e-01]]\n"
     ]
    }
   ],
   "source": [
    "### print('training done')\n",
    "#demonstrate prediction\n",
    "x_input = Xa[0,:,:]\n",
    "x_input=x_input.reshape((1,18,9))\n",
    "yhat = model.predict(x_input,verbose=1)\n",
    "print(yhat)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
