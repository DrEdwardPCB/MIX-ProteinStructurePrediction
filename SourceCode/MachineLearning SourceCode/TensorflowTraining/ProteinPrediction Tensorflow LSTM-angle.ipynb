{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete without error\n"
     ]
    }
   ],
   "source": [
    "##Author: Wong Yuk Ming, Edward\n",
    "##Remarks: it seems using keras does not require us to convert numpy array to tensor, it will done once we fit the data\n",
    "\n",
    "import os \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy import load\n",
    "\n",
    "#loading the file from desire directory\n",
    "path = \"../itasserSx9\" ##<-modify this directory is ok\n",
    "\n",
    "filenames= os.listdir(path)\n",
    "x_List=[]\n",
    "y_List=[]\n",
    "xyfileTuple=[]\n",
    "\n",
    "#since on linux file are not ordered by name, there is no way to guarentee n+1 file is the y file of x file n\n",
    "#there will not be any selection that limit the number of file to process\n",
    "\n",
    "for names in filenames:\n",
    "    if names.endswith(\"-x-.npy\"):\n",
    "        x_List.append(names)\n",
    "    else:\n",
    "        y_List.append(names)\n",
    "\n",
    "##testing purpose\n",
    "#print(x_List)\n",
    "#print(y_List)\n",
    "\n",
    "for xfile in x_List:\n",
    "    for yfile in y_List:\n",
    "        if (xfile.replace('-x-.npy','') == yfile.replace('-y-.npy','')):\n",
    "            xyfileTuple.append((xfile,yfile))\n",
    "            break\n",
    "##testing purpose\n",
    "#print(xyfileTuple)\n",
    "\n",
    "#notice that this cell has no error\n",
    "print('complete without error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "complete without error\n"
     ]
    }
   ],
   "source": [
    "#from filename convert to numpy array \n",
    "##input:  array of tuple contains x file name and y file name, purpose <\"angle\", \"nonangle\">\n",
    "##output: array of tuple contains x numpy array and y numpy array\n",
    "def extractData(tupleToConv, purpose):\n",
    "    outputTensorTuple=[]\n",
    "    i=0\n",
    "    for pair in tupleToConv:\n",
    "        if(purpose=='angle'):\n",
    "            xdatanp=load(path+\"/\"+pair[0])\n",
    "            xdatatf=tf.convert_to_tensor(xdatanp, dtype=np.float32)\n",
    "            \n",
    "            ydatanp=load(path+\"/\"+pair[1])\n",
    "            #print(ydatanp)\n",
    "            ydatanp=ydatanp[:,4:6]\n",
    "            #if(i==0):\n",
    "                #print('from y')\n",
    "                #print(ydatanp)\n",
    "            #ydatatf=tf.convert_to_tensor(xdatanp, dtype=np.float32)\n",
    "            outputTensorTuple.append((xdatanp,ydatanp))\n",
    "            \n",
    "        else:\n",
    "            xdatanp=load(path+\"/\"+pair[0])\n",
    "            xdatatf=tf.convert_to_tensor(xdatanp, dtype=np.float32)\n",
    "            \n",
    "            ydatanp=load(path+\"/\"+pair[1])\n",
    "            ydatanp=ydatanp[:,:4]\n",
    "            if(i==0):\n",
    "                print(ydatanp)\n",
    "            #ydatatf=tf.convert_to_tensor(xdatanp, dtype=np.float32)\n",
    "            outputTensorTuple.append((xdatanp,ydatanp))\n",
    "        \n",
    "        i=i+1\n",
    "            \n",
    "    return outputTensorTuple\n",
    "\n",
    "angleAllData = extractData(xyfileTuple,purpose='angle')\n",
    "ssAllData = extractData(xyfileTuple,purpose='nonangle')\n",
    "\n",
    "#notice that this cell has no error\n",
    "print('complete without error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete without error\n"
     ]
    }
   ],
   "source": [
    "#extract to k-mers where k=18 for each amino acid position\n",
    "#for amino acid at position n, x= n-18:n, y=ny\n",
    "#also add padding\n",
    "\n",
    "\n",
    "angleKmersData=[]\n",
    "ssKmersData=[]\n",
    "\n",
    "#specify how many file want to process in here\n",
    "numFileProcess=100 ##<-modify this for changing -1 = process all file\n",
    "if (numFileProcess==-1):\n",
    "    numFilProcess=len(angleAllData)\n",
    "\n",
    "for i in range(numFileProcess):\n",
    "    thisAngleTuple = angleAllData[i]\n",
    "    thisSSTuple = ssAllData[i]\n",
    "    #orgAngleLenght=thisAngleTuple[0].shape[0]\n",
    "    #orgSSLenght=thisSSTuple[0].shape[0]\n",
    "    thisAngleTuple=(np.insert(thisAngleTuple[0],0,np.zeros((17,9)),axis=0),thisAngleTuple[1])\n",
    "    thisSSTuple=(np.insert(thisSSTuple[0],0,np.zeros((17,9)),axis=0),thisSSTuple[1])\n",
    "    thisAngleTuple=(np.append(thisAngleTuple[0],np.zeros((17,9)),axis=0),thisAngleTuple[1])\n",
    "    thisSSTuple=(np.append(thisSSTuple[0],np.zeros((17,9)),axis=0),thisSSTuple[1])\n",
    "    for j in range(thisAngleTuple[1].shape[0]):\n",
    "        x_vec=None\n",
    "        y_vec=None\n",
    "        x_vec=thisAngleTuple[0][j:j+35,:]\n",
    "        y_vec=thisAngleTuple[1][j,:]\n",
    "        angleKmersData.append((x_vec,y_vec))\n",
    "\n",
    "    for j in range(thisSSTuple[1].shape[0]):\n",
    "        x_vec=None\n",
    "        y_vec=None    \n",
    "        x_vec=thisSSTuple[0][j:j+35,:]\n",
    "        y_vec=thisSSTuple[1][j,:]\n",
    "        ssKmersData.append((x_vec,y_vec))\n",
    "       \n",
    "'''\n",
    "for i in range(numFileProcess):\n",
    "    thisAngleTuple = angleAllData[i]\n",
    "    thisSSTuple = ssAllData[i]\n",
    "    ##reporter\n",
    "    #if(i==0):\n",
    "        #print(thisAngleTuple[0].shape)\n",
    "        #print(thisAngleTuple[0][0:18,:])\n",
    "    for j in range(thisAngleTuple[0].shape[0]):\n",
    "        x_vec=None\n",
    "        y_vec=None\n",
    "        if(j<18):\n",
    "            #print(j)\n",
    "            x_vec=thisAngleTuple[0][0:j+1,:]\n",
    "            x_vec=np.insert(x_vec, 0 , np.zeros([17-j,9]) , axis=0)#adding padding\n",
    "            #print(x_vec)\n",
    "            #print(x_vec.shape)\n",
    "        else:\n",
    "            #print(j)\n",
    "            x_vec=thisAngleTuple[0][j-17:j+1,:]\n",
    "            #if(j==18):\n",
    "            #    print(x_vec)\n",
    "            #print(x_vec.shape)\n",
    "        y_vec=thisAngleTuple[1][j,:]\n",
    "        #x_vec=tf.convert_to_tensor(x_vec, dtype=np.float32)\n",
    "        #y_vec=tf.convert_to_tensor(y_vec, dtype=np.float32)\n",
    "        angleKmersData.append((x_vec,y_vec))\n",
    "    for j in range(thisAngleTuple[0].shape[0]):\n",
    "        x_vec=None\n",
    "        y_vec=None\n",
    "        if(j<18):\n",
    "            #print(j)\n",
    "            x_vec=thisSSTuple[0][0:j+1,:]\n",
    "            x_vec=np.insert(x_vec, 0 , np.zeros([17-j,9]) , axis=0)#adding padding\n",
    "            #print(x_vec)\n",
    "            #print(x_vec.shape)\n",
    "        else:\n",
    "            #print(j)\n",
    "            x_vec=thisSSTuple[0][j-17:j+1,:]\n",
    "            #if(j==18):\n",
    "            #    print(x_vec)\n",
    "            #print(x_vec.shape)\n",
    "        y_vec=thisSSTuple[1][j,:]\n",
    "        #x_vec=tf.convert_to_tensor(x_vec, dtype=np.float32)\n",
    "        #y_vec=tf.convert_to_tensor(y_vec, dtype=np.float32)\n",
    "        ssKmersData.append((x_vec,y_vec))\n",
    "'''\n",
    "'''\n",
    "for tup in angleKmersData:\n",
    "    print(str(tup[0].shape)+\" = \"+str(tup[1].shape))\n",
    "for tup in ssKmersData:\n",
    "    print(str(tup[0].shape)+\" = \"+str(tup[1].shape))\n",
    "'''\n",
    "#notice that this cell has no error\n",
    "print('complete without error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete without error\n"
     ]
    }
   ],
   "source": [
    "#for feeding into the model, it has to be converted from list of 2D array to a 3D array\n",
    "#for angle\n",
    "from math import floor\n",
    "from random import random\n",
    "Xa=list()\n",
    "Ya=list()\n",
    "\n",
    "Xatest=list()\n",
    "Yatest=list()\n",
    "#for secondary structure\n",
    "Xs=list()\n",
    "Ys=list()\n",
    "\n",
    "Xstest=list()\n",
    "Ystest=list()\n",
    "for item in angleKmersData:\n",
    "    splitting=floor(random()*10)\n",
    "    if(splitting>7):\n",
    "        Xatest.append(item[0])\n",
    "        Yatest.append(item[1])\n",
    "    else:\n",
    "        Xa.append(item[0])\n",
    "        Ya.append(item[1])\n",
    "for item in ssKmersData:\n",
    "    splitting=floor(random()*10)\n",
    "    if(splitting>7):\n",
    "        Xstest.append(item[0])\n",
    "        Ystest.append(item[1])\n",
    "    else:\n",
    "        Xs.append(item[0])\n",
    "        Ys.append(item[1])\n",
    "\n",
    "# checking\n",
    "#for i in range(len(Xa)):\n",
    "#\tprint(Xa[i], Ya[i])\n",
    "Xa=np.array(Xa) #[samples=S timestamp=18 features=9]\n",
    "Ya=np.array(Ya) #[sample=S features=2]\n",
    "Xs=np.array(Xs)\n",
    "Ys=np.array(Ys)\n",
    "\n",
    "Xatest=np.array(Xatest) #[samples=S timestamp=18 features=9]\n",
    "Yatest=np.array(Yatest) #[sample=S features=2]\n",
    "Xstest=np.array(Xstest)\n",
    "Ystest=np.array(Ystest)\n",
    "\n",
    "#print(Xa.shape)\n",
    "#Xa=Xa.reshape((Xa.shape[0],Xa.shape[1],9))\n",
    "#print(Ya.shape)\n",
    "#print(Xa.shape)\n",
    "\n",
    "#notice that this cell has no error\n",
    "print('complete without error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete without error\n"
     ]
    }
   ],
   "source": [
    "#import all things needed for tensorflow LSTM network\n",
    "#this is for predicting the next number\n",
    "from tensorflow.keras.models import Sequential #Sequential model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Flatten #CNN\n",
    "from tensorflow.keras.layers import TimeDistributed #CNN\n",
    "from tensorflow.keras.layers import Conv1D       #CNN\n",
    "from tensorflow.keras.layers import MaxPooling1D #CNN\n",
    "\n",
    "#notice that this cell has no error\n",
    "print('complete without error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete without error\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from tensorflow.keras.backend import sigmoid\n",
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.layers import Activation\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "'''\n",
    "#notice that this cell has no error\n",
    "print('complete without error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 41784 samples, validate on 10810 samples\n",
      "Epoch 1/100\n",
      " 1000/41784 [..............................] - ETA: 4:35 - loss: 8891.3506WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.102395). Check your callbacks.\n",
      "41784/41784 [==============================] - 24s 570us/sample - loss: 7967.1508 - val_loss: 7267.5129\n",
      "Epoch 2/100\n",
      "41784/41784 [==============================] - 17s 403us/sample - loss: 7076.7743 - val_loss: 6765.4410\n",
      "Epoch 3/100\n",
      "41784/41784 [==============================] - 17s 402us/sample - loss: 6664.4374 - val_loss: 6448.8147\n",
      "Epoch 4/100\n",
      "41784/41784 [==============================] - 16s 388us/sample - loss: 6400.8970 - val_loss: 6244.1710\n",
      "Epoch 5/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 6228.3530 - val_loss: 6111.2423\n",
      "Epoch 6/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 6116.3927 - val_loss: 6027.6594\n",
      "Epoch 7/100\n",
      "41784/41784 [==============================] - 16s 379us/sample - loss: 6045.5516 - val_loss: 5975.3996\n",
      "Epoch 8/100\n",
      "41784/41784 [==============================] - 16s 380us/sample - loss: 5991.6287 - val_loss: 5923.8051\n",
      "Epoch 9/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5959.0897 - val_loss: 5918.7156\n",
      "Epoch 10/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5943.7947 - val_loss: 5908.3836\n",
      "Epoch 11/100\n",
      "41784/41784 [==============================] - 16s 379us/sample - loss: 5934.2896 - val_loss: 5902.2489\n",
      "Epoch 12/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5916.3932 - val_loss: 5858.0861\n",
      "Epoch 13/100\n",
      "41784/41784 [==============================] - 16s 380us/sample - loss: 5870.2254 - val_loss: 5837.2797\n",
      "Epoch 14/100\n",
      "41784/41784 [==============================] - 16s 379us/sample - loss: 5857.1365 - val_loss: 5833.7121\n",
      "Epoch 15/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5854.3977 - val_loss: 5833.0314\n",
      "Epoch 16/100\n",
      "41784/41784 [==============================] - 16s 379us/sample - loss: 5853.4749 - val_loss: 5832.9196\n",
      "Epoch 17/100\n",
      "41784/41784 [==============================] - 16s 379us/sample - loss: 5853.1848 - val_loss: 5832.9248\n",
      "Epoch 18/100\n",
      "41784/41784 [==============================] - 16s 379us/sample - loss: 5852.9745 - val_loss: 5833.0147\n",
      "Epoch 19/100\n",
      "41784/41784 [==============================] - 16s 380us/sample - loss: 5852.9333 - val_loss: 5833.0947\n",
      "Epoch 20/100\n",
      "41784/41784 [==============================] - 16s 379us/sample - loss: 5852.8602 - val_loss: 5833.1361\n",
      "Epoch 21/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5852.8495 - val_loss: 5833.1179\n",
      "Epoch 22/100\n",
      "41784/41784 [==============================] - 16s 377us/sample - loss: 5852.8312 - val_loss: 5833.1433\n",
      "Epoch 23/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5852.8877 - val_loss: 5833.0220\n",
      "Epoch 24/100\n",
      "41784/41784 [==============================] - 16s 380us/sample - loss: 5852.7463 - val_loss: 5833.0294\n",
      "Epoch 25/100\n",
      "41784/41784 [==============================] - 16s 379us/sample - loss: 5852.7026 - val_loss: 5832.5362\n",
      "Epoch 26/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5852.5238 - val_loss: 5832.6145\n",
      "Epoch 27/100\n",
      "41784/41784 [==============================] - 16s 380us/sample - loss: 5852.4370 - val_loss: 5832.6910\n",
      "Epoch 28/100\n",
      "41784/41784 [==============================] - 16s 381us/sample - loss: 5852.3617 - val_loss: 5832.6779\n",
      "Epoch 29/100\n",
      "41784/41784 [==============================] - 16s 380us/sample - loss: 5852.2796 - val_loss: 5832.1083\n",
      "Epoch 30/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5873.0361 - val_loss: 5910.9654\n",
      "Epoch 31/100\n",
      "41784/41784 [==============================] - 16s 377us/sample - loss: 5935.1148 - val_loss: 5882.0019\n",
      "Epoch 32/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5902.8804 - val_loss: 5869.0158\n",
      "Epoch 33/100\n",
      "41784/41784 [==============================] - 16s 377us/sample - loss: 5896.5349 - val_loss: 5863.7390\n",
      "Epoch 34/100\n",
      "41784/41784 [==============================] - 16s 377us/sample - loss: 5875.9649 - val_loss: 5848.5040\n",
      "Epoch 35/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5870.3182 - val_loss: 5847.6659\n",
      "Epoch 36/100\n",
      "41784/41784 [==============================] - 16s 377us/sample - loss: 5869.5888 - val_loss: 5846.7263\n",
      "Epoch 37/100\n",
      "41784/41784 [==============================] - 16s 377us/sample - loss: 5869.1833 - val_loss: 5845.9515\n",
      "Epoch 38/100\n",
      "41784/41784 [==============================] - 16s 381us/sample - loss: 5867.9084 - val_loss: 5845.2333\n",
      "Epoch 39/100\n",
      "41784/41784 [==============================] - 16s 379us/sample - loss: 5866.8763 - val_loss: 5843.7417\n",
      "Epoch 40/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5865.7565 - val_loss: 5842.7654\n",
      "Epoch 41/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5863.2218 - val_loss: 5839.7276\n",
      "Epoch 42/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5861.4041 - val_loss: 5839.0158\n",
      "Epoch 43/100\n",
      "41784/41784 [==============================] - 16s 379us/sample - loss: 5859.6636 - val_loss: 5837.7036\n",
      "Epoch 44/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5858.4663 - val_loss: 5837.7754\n",
      "Epoch 45/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5858.0491 - val_loss: 5836.9815\n",
      "Epoch 46/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5856.5528 - val_loss: 5836.0071\n",
      "Epoch 47/100\n",
      "41784/41784 [==============================] - 16s 380us/sample - loss: 5855.5365 - val_loss: 5834.5188\n",
      "Epoch 48/100\n",
      "41784/41784 [==============================] - 16s 380us/sample - loss: 5854.8516 - val_loss: 5834.0189\n",
      "Epoch 49/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5854.2938 - val_loss: 5833.6117\n",
      "Epoch 50/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5853.2761 - val_loss: 5833.1986\n",
      "Epoch 51/100\n",
      "41784/41784 [==============================] - 16s 379us/sample - loss: 5852.4497 - val_loss: 5832.2331\n",
      "Epoch 52/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5851.9388 - val_loss: 5830.9575\n",
      "Epoch 53/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5850.8690 - val_loss: 5831.9985\n",
      "Epoch 54/100\n",
      "41784/41784 [==============================] - 16s 379us/sample - loss: 5848.4784 - val_loss: 5829.1445\n",
      "Epoch 55/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5854.1157 - val_loss: 5900.4789\n",
      "Epoch 56/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5897.7106 - val_loss: 5858.4795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "41784/41784 [==============================] - 16s 379us/sample - loss: 5884.4852 - val_loss: 5859.1938\n",
      "Epoch 58/100\n",
      "41784/41784 [==============================] - 16s 379us/sample - loss: 5864.5197 - val_loss: 5827.4071\n",
      "Epoch 59/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5849.8126 - val_loss: 5823.5358\n",
      "Epoch 60/100\n",
      "41784/41784 [==============================] - 16s 377us/sample - loss: 5849.1573 - val_loss: 5826.5783\n",
      "Epoch 61/100\n",
      "41784/41784 [==============================] - 16s 379us/sample - loss: 5844.0261 - val_loss: 5816.8705\n",
      "Epoch 62/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5838.8652 - val_loss: 5812.9779\n",
      "Epoch 63/100\n",
      "41784/41784 [==============================] - 16s 379us/sample - loss: 5837.9348 - val_loss: 5809.0661\n",
      "Epoch 64/100\n",
      "41784/41784 [==============================] - 16s 380us/sample - loss: 5827.1007 - val_loss: 5803.5684\n",
      "Epoch 65/100\n",
      "41784/41784 [==============================] - 16s 377us/sample - loss: 5825.5410 - val_loss: 5809.1245\n",
      "Epoch 66/100\n",
      "41784/41784 [==============================] - 16s 379us/sample - loss: 5821.0329 - val_loss: 5799.3572\n",
      "Epoch 67/100\n",
      "41784/41784 [==============================] - 16s 383us/sample - loss: 5816.1512 - val_loss: 5793.8387\n",
      "Epoch 68/100\n",
      "41784/41784 [==============================] - 16s 389us/sample - loss: 5810.0842 - val_loss: 5787.8273\n",
      "Epoch 69/100\n",
      "41784/41784 [==============================] - 16s 374us/sample - loss: 5817.5198 - val_loss: 5789.5373\n",
      "Epoch 70/100\n",
      "41784/41784 [==============================] - 16s 374us/sample - loss: 5805.0762 - val_loss: 5793.3677\n",
      "Epoch 71/100\n",
      "41784/41784 [==============================] - 16s 375us/sample - loss: 5816.7604 - val_loss: 5789.1170\n",
      "Epoch 72/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5795.4241 - val_loss: 5777.8162\n",
      "Epoch 73/100\n",
      "41784/41784 [==============================] - 16s 375us/sample - loss: 5792.6170 - val_loss: 5761.6561\n",
      "Epoch 74/100\n",
      "41784/41784 [==============================] - 16s 373us/sample - loss: 5787.4304 - val_loss: 5776.6843\n",
      "Epoch 75/100\n",
      "41784/41784 [==============================] - 16s 376us/sample - loss: 5794.6223 - val_loss: 5779.2474\n",
      "Epoch 76/100\n",
      "41784/41784 [==============================] - 16s 374us/sample - loss: 5776.8591 - val_loss: 5764.1829\n",
      "Epoch 77/100\n",
      "41784/41784 [==============================] - 16s 374us/sample - loss: 5774.8693 - val_loss: 5740.1376\n",
      "Epoch 78/100\n",
      "41784/41784 [==============================] - 16s 374us/sample - loss: 5748.1897 - val_loss: 5727.6840\n",
      "Epoch 79/100\n",
      "41784/41784 [==============================] - 16s 377us/sample - loss: 5744.9640 - val_loss: 5723.1129\n",
      "Epoch 80/100\n",
      "41784/41784 [==============================] - 16s 375us/sample - loss: 5744.3193 - val_loss: 5736.4413\n",
      "Epoch 81/100\n",
      "41784/41784 [==============================] - 16s 377us/sample - loss: 5741.2075 - val_loss: 5724.7979\n",
      "Epoch 82/100\n",
      "41784/41784 [==============================] - 16s 377us/sample - loss: 5727.4574 - val_loss: 5718.0248\n",
      "Epoch 83/100\n",
      "41784/41784 [==============================] - 16s 376us/sample - loss: 5720.1405 - val_loss: 5709.9473\n",
      "Epoch 84/100\n",
      "41784/41784 [==============================] - 16s 376us/sample - loss: 5720.5294 - val_loss: 5694.8080\n",
      "Epoch 85/100\n",
      "41784/41784 [==============================] - 16s 377us/sample - loss: 5710.8761 - val_loss: 5728.3626\n",
      "Epoch 86/100\n",
      "41784/41784 [==============================] - 16s 376us/sample - loss: 5709.6663 - val_loss: 5710.3465\n",
      "Epoch 87/100\n",
      "41784/41784 [==============================] - 16s 375us/sample - loss: 5697.6066 - val_loss: 5738.6820\n",
      "Epoch 88/100\n",
      "41784/41784 [==============================] - 16s 374us/sample - loss: 5691.1245 - val_loss: 5674.4166\n",
      "Epoch 89/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5690.3873 - val_loss: 5677.5432\n",
      "Epoch 90/100\n",
      "41784/41784 [==============================] - 16s 373us/sample - loss: 5693.4101 - val_loss: 5698.0407\n",
      "Epoch 91/100\n",
      "41784/41784 [==============================] - 16s 375us/sample - loss: 5667.0638 - val_loss: 5658.5412\n",
      "Epoch 92/100\n",
      "41784/41784 [==============================] - 16s 375us/sample - loss: 5662.9149 - val_loss: 5631.5915\n",
      "Epoch 93/100\n",
      "41784/41784 [==============================] - 16s 375us/sample - loss: 5669.7614 - val_loss: 5756.1385\n",
      "Epoch 94/100\n",
      "41784/41784 [==============================] - 16s 377us/sample - loss: 5723.5332 - val_loss: 5673.2803\n",
      "Epoch 95/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5663.8133 - val_loss: 5690.6024\n",
      "Epoch 96/100\n",
      "41784/41784 [==============================] - 16s 378us/sample - loss: 5663.8809 - val_loss: 5645.4521\n",
      "Epoch 97/100\n",
      "41784/41784 [==============================] - 16s 374us/sample - loss: 5663.6788 - val_loss: 5636.4250\n",
      "Epoch 98/100\n",
      "41784/41784 [==============================] - 16s 377us/sample - loss: 5650.4712 - val_loss: 5679.5234\n",
      "Epoch 99/100\n",
      "41784/41784 [==============================] - 16s 376us/sample - loss: 5638.2184 - val_loss: 5612.4560\n",
      "Epoch 100/100\n",
      "41784/41784 [==============================] - 16s 375us/sample - loss: 5608.6269 - val_loss: 5606.2312\n",
      "complete without error\n"
     ]
    }
   ],
   "source": [
    "##setting up callback for tensor board\n",
    "# Load the TensorBoard notebook extension\n",
    "\n",
    "\n",
    "##saving checkpoint\n",
    "model_checkpoint_callback1 = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='./saved_model/checkpoint/max',\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    "    \n",
    ")\n",
    "model_checkpoint_callback2 = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='./saved_model/checkpoint/e50',\n",
    "    save_weights_only=False,\n",
    "    save_freq=50\n",
    ")\n",
    "##setting up model\n",
    "\n",
    "# for CNN-LSTM reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "# if not using CNN, just comment this line\n",
    "#Xa=Xa.reshape(Xa.shape[0],Xa.shape[1],Xa.shape[2],1)\n",
    "\n",
    "#model\n",
    "model = Sequential()\n",
    "#model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, 9,1)))\n",
    "#model.add(TimeDistributed(MaxPooling1D(pool_size=9)))\n",
    "#model.add(TimeDistributed(Flatten()))\n",
    "model.add(Bidirectional(LSTM(100, activation='tanh',return_sequences=True, recurrent_activation='sigmoid', use_bias=True , input_shape=(35,9))))\n",
    "model.add(Bidirectional(LSTM(100, activation='tanh',return_sequences=True, recurrent_activation='sigmoid', use_bias=True )))\n",
    "#model.add(Bidirectional(LSTM(50, activation='tanh',return_sequences=True, recurrent_activation='sigmoid', use_bias=True )))\n",
    "model.add(Bidirectional(LSTM(100, activation='tanh',  recurrent_activation='sigmoid', use_bias=True )))\n",
    "model.add(Dense(2))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "#fitting the model\n",
    "#fitting the model\n",
    "model.fit(x=Xa,\n",
    "          y=Ya,\n",
    "          batch_size=1000,\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          callbacks=[\n",
    "              #tensorboard_callback,\n",
    "              model_checkpoint_callback1,model_checkpoint_callback2 ],\n",
    "          validation_data=(Xatest,Yatest)\n",
    "         )\n",
    "\n",
    "model.save('./saved_model/angleModel')\n",
    "\n",
    "#notice that this cell has no error\n",
    "print('complete without error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training done\n",
      "1/1 [==============================] - 1s 681ms/sample\n",
      "[[16.647606 38.323067]]\n"
     ]
    }
   ],
   "source": [
    "print('training done')\n",
    "#demonstrate prediction\n",
    "x_input = Xa[0,:,:]\n",
    "x_input=x_input.reshape((1,35,9))\n",
    "yhat = model.predict(x_input,verbose=1)\n",
    "print(yhat)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
