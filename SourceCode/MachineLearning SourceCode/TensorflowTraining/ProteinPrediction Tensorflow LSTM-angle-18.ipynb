{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete without error\n"
     ]
    }
   ],
   "source": [
    "##Author: Wong Yuk Ming, Edward\n",
    "##Remarks: it seems using keras does not require us to convert numpy array to tensor, it will done once we fit the data\n",
    "\n",
    "import os \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy import load\n",
    "\n",
    "#loading the file from desire directory\n",
    "path = \"../itasserSx9\" ##<-modify this directory is ok\n",
    "\n",
    "filenames= os.listdir(path)\n",
    "x_List=[]\n",
    "y_List=[]\n",
    "xyfileTuple=[]\n",
    "\n",
    "#since on linux file are not ordered by name, there is no way to guarentee n+1 file is the y file of x file n\n",
    "#there will not be any selection that limit the number of file to process\n",
    "\n",
    "for names in filenames:\n",
    "    if names.endswith(\"-x-.npy\"):\n",
    "        x_List.append(names)\n",
    "    else:\n",
    "        y_List.append(names)\n",
    "\n",
    "##testing purpose\n",
    "#print(x_List)\n",
    "#print(y_List)\n",
    "\n",
    "for xfile in x_List:\n",
    "    for yfile in y_List:\n",
    "        if (xfile.replace('-x-.npy','') == yfile.replace('-y-.npy','')):\n",
    "            xyfileTuple.append((xfile,yfile))\n",
    "            break\n",
    "##testing purpose\n",
    "#print(xyfileTuple)\n",
    "\n",
    "#notice that this cell has no error\n",
    "print('complete without error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "complete without error\n"
     ]
    }
   ],
   "source": [
    "#from filename convert to numpy array \n",
    "##input:  array of tuple contains x file name and y file name, purpose <\"angle\", \"nonangle\">\n",
    "##output: array of tuple contains x numpy array and y numpy array\n",
    "def extractData(tupleToConv, purpose):\n",
    "    outputTensorTuple=[]\n",
    "    i=0\n",
    "    for pair in tupleToConv:\n",
    "        if(purpose=='angle'):\n",
    "            xdatanp=load(path+\"/\"+pair[0])\n",
    "            xdatatf=tf.convert_to_tensor(xdatanp, dtype=np.float32)\n",
    "            \n",
    "            ydatanp=load(path+\"/\"+pair[1])\n",
    "            #print(ydatanp)\n",
    "            ydatanp=ydatanp[:,4:6]\n",
    "            #if(i==0):\n",
    "                #print('from y')\n",
    "                #print(ydatanp)\n",
    "            #ydatatf=tf.convert_to_tensor(xdatanp, dtype=np.float32)\n",
    "            outputTensorTuple.append((xdatanp,ydatanp))\n",
    "            \n",
    "        else:\n",
    "            xdatanp=load(path+\"/\"+pair[0])\n",
    "            xdatatf=tf.convert_to_tensor(xdatanp, dtype=np.float32)\n",
    "            \n",
    "            ydatanp=load(path+\"/\"+pair[1])\n",
    "            ydatanp=ydatanp[:,:4]\n",
    "            if(i==0):\n",
    "                print(ydatanp)\n",
    "            #ydatatf=tf.convert_to_tensor(xdatanp, dtype=np.float32)\n",
    "            outputTensorTuple.append((xdatanp,ydatanp))\n",
    "        \n",
    "        i=i+1\n",
    "            \n",
    "    return outputTensorTuple\n",
    "\n",
    "angleAllData = extractData(xyfileTuple,purpose='angle')\n",
    "ssAllData = extractData(xyfileTuple,purpose='nonangle')\n",
    "\n",
    "#notice that this cell has no error\n",
    "print('complete without error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete without error\n"
     ]
    }
   ],
   "source": [
    "#extract to k-mers where k=18 for each amino acid position\n",
    "#for amino acid at position n, x= n-18:n, y=ny\n",
    "#also add padding\n",
    "\n",
    "\n",
    "angleKmersData=[]\n",
    "ssKmersData=[]\n",
    "\n",
    "#specify how many file want to process in here\n",
    "numFileProcess=100 ##<-modify this for changing -1 = process all file\n",
    "if (numFileProcess==-1):\n",
    "    numFilProcess=len(angleAllData)\n",
    "\n",
    "for i in range(numFileProcess):\n",
    "    thisAngleTuple = angleAllData[i]\n",
    "    thisSSTuple = ssAllData[i]\n",
    "    ##reporter\n",
    "    #if(i==0):\n",
    "        #print(thisAngleTuple[0].shape)\n",
    "        #print(thisAngleTuple[0][0:18,:])\n",
    "    for j in range(thisAngleTuple[0].shape[0]):\n",
    "        x_vec=None\n",
    "        y_vec=None\n",
    "        if(j<18):\n",
    "            #print(j)\n",
    "            x_vec=thisAngleTuple[0][0:j+1,:]\n",
    "            x_vec=np.insert(x_vec, 0 , np.zeros([17-j,9]) , axis=0)#adding padding\n",
    "            #print(x_vec)\n",
    "            #print(x_vec.shape)\n",
    "        else:\n",
    "            #print(j)\n",
    "            x_vec=thisAngleTuple[0][j-17:j+1,:]\n",
    "            #if(j==18):\n",
    "            #    print(x_vec)\n",
    "            #print(x_vec.shape)\n",
    "        y_vec=thisAngleTuple[1][j,:]\n",
    "        #x_vec=tf.convert_to_tensor(x_vec, dtype=np.float32)\n",
    "        #y_vec=tf.convert_to_tensor(y_vec, dtype=np.float32)\n",
    "        angleKmersData.append((x_vec,y_vec))\n",
    "    for j in range(thisAngleTuple[0].shape[0]):\n",
    "        x_vec=None\n",
    "        y_vec=None\n",
    "        if(j<18):\n",
    "            #print(j)\n",
    "            x_vec=thisSSTuple[0][0:j+1,:]\n",
    "            x_vec=np.insert(x_vec, 0 , np.zeros([17-j,9]) , axis=0)#adding padding\n",
    "            #print(x_vec)\n",
    "            #print(x_vec.shape)\n",
    "        else:\n",
    "            #print(j)\n",
    "            x_vec=thisSSTuple[0][j-17:j+1,:]\n",
    "            #if(j==18):\n",
    "            #    print(x_vec)\n",
    "            #print(x_vec.shape)\n",
    "        y_vec=thisSSTuple[1][j,:]\n",
    "        #x_vec=tf.convert_to_tensor(x_vec, dtype=np.float32)\n",
    "        #y_vec=tf.convert_to_tensor(y_vec, dtype=np.float32)\n",
    "        ssKmersData.append((x_vec,y_vec))\n",
    "\n",
    "'''\n",
    "for tup in angleKmersData:\n",
    "    print(str(tup[0].shape)+\" = \"+str(tup[1].shape))\n",
    "for tup in ssKmersData:\n",
    "    print(str(tup[0].shape)+\" = \"+str(tup[1].shape))\n",
    "'''\n",
    "#notice that this cell has no error\n",
    "print('complete without error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete without error\n"
     ]
    }
   ],
   "source": [
    "#for feeding into the model, it has to be converted from list of 2D array to a 3D array\n",
    "#for angle\n",
    "from math import floor\n",
    "from random import random\n",
    "Xa=list()\n",
    "Ya=list()\n",
    "\n",
    "Xatest=list()\n",
    "Yatest=list()\n",
    "#for secondary structure\n",
    "Xs=list()\n",
    "Ys=list()\n",
    "\n",
    "Xstest=list()\n",
    "Ystest=list()\n",
    "for item in angleKmersData:\n",
    "    splitting=floor(random()*10)\n",
    "    if(splitting>7):\n",
    "        Xatest.append(item[0])\n",
    "        Yatest.append(item[1])\n",
    "    else:\n",
    "        Xa.append(item[0])\n",
    "        Ya.append(item[1])\n",
    "for item in ssKmersData:\n",
    "    splitting=floor(random()*10)\n",
    "    if(splitting>7):\n",
    "        Xstest.append(item[0])\n",
    "        Ystest.append(item[1])\n",
    "    else:\n",
    "        Xs.append(item[0])\n",
    "        Ys.append(item[1])\n",
    "\n",
    "# checking\n",
    "#for i in range(len(Xa)):\n",
    "#\tprint(Xa[i], Ya[i])\n",
    "Xa=np.array(Xa) #[samples=S timestamp=18 features=9]\n",
    "Ya=np.array(Ya) #[sample=S features=2]\n",
    "Xs=np.array(Xs)\n",
    "Ys=np.array(Ys)\n",
    "\n",
    "Xatest=np.array(Xatest) #[samples=S timestamp=18 features=9]\n",
    "Yatest=np.array(Yatest) #[sample=S features=2]\n",
    "Xstest=np.array(Xstest)\n",
    "Ystest=np.array(Ystest)\n",
    "\n",
    "#print(Xa.shape)\n",
    "#Xa=Xa.reshape((Xa.shape[0],Xa.shape[1],9))\n",
    "#print(Ya.shape)\n",
    "#print(Xa.shape)\n",
    "\n",
    "#notice that this cell has no error\n",
    "print('complete without error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete without error\n"
     ]
    }
   ],
   "source": [
    "#import all things needed for tensorflow LSTM network\n",
    "#this is for predicting the next number\n",
    "from tensorflow.keras.models import Sequential #Sequential model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Flatten #CNN\n",
    "from tensorflow.keras.layers import TimeDistributed #CNN\n",
    "from tensorflow.keras.layers import Conv1D       #CNN\n",
    "from tensorflow.keras.layers import MaxPooling1D #CNN\n",
    "\n",
    "#notice that this cell has no error\n",
    "print('complete without error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete without error\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from tensorflow.keras.backend import sigmoid\n",
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.layers import Activation\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "'''\n",
    "#notice that this cell has no error\n",
    "print('complete without error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Server\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 42109 samples, validate on 10485 samples\n",
      "Epoch 1/100\n",
      " 1000/42109 [..............................] - ETA: 5:41 - loss: 9107.3193WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.266770). Check your callbacks.\n",
      "42109/42109 [==============================] - 32s 749us/sample - loss: 6327.1280 - val_loss: 5789.6092\n",
      "Epoch 2/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 5764.7588 - val_loss: 5552.8763\n",
      "Epoch 3/100\n",
      "42109/42109 [==============================] - 23s 548us/sample - loss: 5498.6515 - val_loss: 5411.2248\n",
      "Epoch 4/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 5396.3996 - val_loss: 5462.1439\n",
      "Epoch 5/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 5373.0039 - val_loss: 5278.4679\n",
      "Epoch 6/100\n",
      "42109/42109 [==============================] - 23s 547us/sample - loss: 5315.0369 - val_loss: 5275.9650\n",
      "Epoch 7/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 5296.5927 - val_loss: 5257.1664\n",
      "Epoch 8/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 5250.9472 - val_loss: 5298.5002\n",
      "Epoch 9/100\n",
      "42109/42109 [==============================] - 23s 547us/sample - loss: 5233.6488 - val_loss: 5191.0231\n",
      "Epoch 10/100\n",
      "42109/42109 [==============================] - 23s 545us/sample - loss: 5219.1863 - val_loss: 5198.5644\n",
      "Epoch 11/100\n",
      "42109/42109 [==============================] - 23s 547us/sample - loss: 5208.3804 - val_loss: 5164.6965\n",
      "Epoch 12/100\n",
      "42109/42109 [==============================] - 23s 550us/sample - loss: 5183.1734 - val_loss: 5158.8356\n",
      "Epoch 13/100\n",
      "42109/42109 [==============================] - 23s 545us/sample - loss: 5147.4388 - val_loss: 5195.4862\n",
      "Epoch 14/100\n",
      "42109/42109 [==============================] - 23s 548us/sample - loss: 5148.5528 - val_loss: 5185.7401\n",
      "Epoch 15/100\n",
      "42109/42109 [==============================] - 23s 552us/sample - loss: 5130.1080 - val_loss: 5137.5054\n",
      "Epoch 16/100\n",
      "42109/42109 [==============================] - 23s 548us/sample - loss: 5134.6053 - val_loss: 5147.5701\n",
      "Epoch 17/100\n",
      "42109/42109 [==============================] - 23s 545us/sample - loss: 5098.2846 - val_loss: 5177.0127\n",
      "Epoch 18/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 5081.9672 - val_loss: 5092.8149\n",
      "Epoch 19/100\n",
      "42109/42109 [==============================] - 23s 545us/sample - loss: 5046.7723 - val_loss: 5089.5822\n",
      "Epoch 20/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 5014.2341 - val_loss: 5095.8979\n",
      "Epoch 21/100\n",
      "42109/42109 [==============================] - 23s 545us/sample - loss: 4997.7660 - val_loss: 5159.9592\n",
      "Epoch 22/100\n",
      "42109/42109 [==============================] - 23s 547us/sample - loss: 4965.4762 - val_loss: 5009.1241\n",
      "Epoch 23/100\n",
      "42109/42109 [==============================] - 23s 544us/sample - loss: 4956.9463 - val_loss: 5012.4101\n",
      "Epoch 24/100\n",
      "42109/42109 [==============================] - 23s 544us/sample - loss: 4893.5216 - val_loss: 5020.9088\n",
      "Epoch 25/100\n",
      "42109/42109 [==============================] - 23s 545us/sample - loss: 4881.2366 - val_loss: 5199.3751\n",
      "Epoch 26/100\n",
      "42109/42109 [==============================] - 23s 552us/sample - loss: 4884.5186 - val_loss: 5121.7412\n",
      "Epoch 27/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 4828.4177 - val_loss: 4974.7192\n",
      "Epoch 28/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 4791.1641 - val_loss: 4976.7319\n",
      "Epoch 29/100\n",
      "42109/42109 [==============================] - 23s 550us/sample - loss: 4736.7300 - val_loss: 5080.8491\n",
      "Epoch 30/100\n",
      "42109/42109 [==============================] - 23s 545us/sample - loss: 4690.5001 - val_loss: 4912.0282\n",
      "Epoch 31/100\n",
      "42109/42109 [==============================] - 23s 543us/sample - loss: 4640.7514 - val_loss: 5157.8474\n",
      "Epoch 32/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 4696.0703 - val_loss: 4950.6485\n",
      "Epoch 33/100\n",
      "42109/42109 [==============================] - 23s 545us/sample - loss: 4561.0538 - val_loss: 4869.4162\n",
      "Epoch 34/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 4501.5917 - val_loss: 4882.8834\n",
      "Epoch 35/100\n",
      "42109/42109 [==============================] - 23s 545us/sample - loss: 4472.1623 - val_loss: 4830.8676\n",
      "Epoch 36/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 4416.4628 - val_loss: 5192.1764\n",
      "Epoch 37/100\n",
      "42109/42109 [==============================] - 23s 545us/sample - loss: 4414.9478 - val_loss: 5191.2095\n",
      "Epoch 38/100\n",
      "42109/42109 [==============================] - 23s 548us/sample - loss: 4378.1556 - val_loss: 4807.5577\n",
      "Epoch 39/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 4256.1030 - val_loss: 5252.8844\n",
      "Epoch 40/100\n",
      "42109/42109 [==============================] - 23s 548us/sample - loss: 4254.2744 - val_loss: 4771.4774\n",
      "Epoch 41/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 4183.7904 - val_loss: 4764.0292\n",
      "Epoch 42/100\n",
      "42109/42109 [==============================] - 23s 544us/sample - loss: 4100.4836 - val_loss: 4797.6586\n",
      "Epoch 43/100\n",
      "42109/42109 [==============================] - 23s 547us/sample - loss: 4078.7098 - val_loss: 4729.9019\n",
      "Epoch 44/100\n",
      "42109/42109 [==============================] - 23s 542us/sample - loss: 4008.3935 - val_loss: 4750.4455\n",
      "Epoch 45/100\n",
      "42109/42109 [==============================] - 23s 545us/sample - loss: 4098.1630 - val_loss: 4676.8861\n",
      "Epoch 46/100\n",
      "42109/42109 [==============================] - 23s 544us/sample - loss: 3961.5156 - val_loss: 5062.3469\n",
      "Epoch 47/100\n",
      "42109/42109 [==============================] - 23s 545us/sample - loss: 3943.9366 - val_loss: 4761.7211\n",
      "Epoch 48/100\n",
      "42109/42109 [==============================] - 23s 545us/sample - loss: 3848.2897 - val_loss: 4714.4307\n",
      "Epoch 49/100\n",
      "42109/42109 [==============================] - 23s 547us/sample - loss: 3770.8468 - val_loss: 4731.2278\n",
      "Epoch 50/100\n",
      "42109/42109 [==============================] - 23s 545us/sample - loss: 3733.5758 - val_loss: 4682.3071\n",
      "Epoch 51/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 3646.4306 - val_loss: 4702.1566\n",
      "Epoch 52/100\n",
      "42109/42109 [==============================] - 23s 544us/sample - loss: 3682.0495 - val_loss: 4750.3772\n",
      "Epoch 53/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 3557.9273 - val_loss: 4692.5925\n",
      "Epoch 54/100\n",
      "42109/42109 [==============================] - 23s 543us/sample - loss: 3519.9962 - val_loss: 4800.3904\n",
      "Epoch 55/100\n",
      "42109/42109 [==============================] - 23s 544us/sample - loss: 3453.8776 - val_loss: 4787.9712\n",
      "Epoch 56/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 3365.4939 - val_loss: 4759.9384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "42109/42109 [==============================] - 23s 549us/sample - loss: 3368.6452 - val_loss: 4747.5988\n",
      "Epoch 58/100\n",
      "42109/42109 [==============================] - 23s 545us/sample - loss: 3278.9952 - val_loss: 4752.2668\n",
      "Epoch 59/100\n",
      "42109/42109 [==============================] - 23s 544us/sample - loss: 3222.6262 - val_loss: 5740.0239\n",
      "Epoch 60/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 3304.6396 - val_loss: 4936.5393\n",
      "Epoch 61/100\n",
      "42109/42109 [==============================] - 23s 545us/sample - loss: 3068.5952 - val_loss: 5121.5793\n",
      "Epoch 62/100\n",
      "42109/42109 [==============================] - 23s 545us/sample - loss: 3046.8621 - val_loss: 5037.5394\n",
      "Epoch 63/100\n",
      "42109/42109 [==============================] - 23s 544us/sample - loss: 2911.8114 - val_loss: 4927.3545\n",
      "Epoch 64/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 2977.3393 - val_loss: 4952.6211\n",
      "Epoch 65/100\n",
      "42109/42109 [==============================] - 23s 545us/sample - loss: 2886.7295 - val_loss: 5165.4570\n",
      "Epoch 66/100\n",
      "42109/42109 [==============================] - 23s 547us/sample - loss: 2897.1945 - val_loss: 4882.4142\n",
      "Epoch 67/100\n",
      "42109/42109 [==============================] - 23s 544us/sample - loss: 2789.1701 - val_loss: 4919.0979\n",
      "Epoch 68/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 2691.5649 - val_loss: 4978.6230\n",
      "Epoch 69/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 2632.2322 - val_loss: 4907.8252\n",
      "Epoch 70/100\n",
      "42109/42109 [==============================] - 23s 543us/sample - loss: 2582.0879 - val_loss: 5005.8863\n",
      "Epoch 71/100\n",
      "42109/42109 [==============================] - 23s 543us/sample - loss: 2511.8377 - val_loss: 5042.2245\n",
      "Epoch 72/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 2411.2517 - val_loss: 5033.3837\n",
      "Epoch 73/100\n",
      "42109/42109 [==============================] - 23s 548us/sample - loss: 2436.5020 - val_loss: 5289.5871\n",
      "Epoch 74/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 2362.6688 - val_loss: 4980.1399\n",
      "Epoch 75/100\n",
      "42109/42109 [==============================] - 23s 545us/sample - loss: 2288.5285 - val_loss: 5033.7225\n",
      "Epoch 76/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 2218.0686 - val_loss: 5157.5433\n",
      "Epoch 77/100\n",
      "42109/42109 [==============================] - 23s 545us/sample - loss: 2229.2391 - val_loss: 5132.9448\n",
      "Epoch 78/100\n",
      "42109/42109 [==============================] - 23s 544us/sample - loss: 2112.5641 - val_loss: 5313.6739\n",
      "Epoch 79/100\n",
      "42109/42109 [==============================] - 23s 545us/sample - loss: 2053.2323 - val_loss: 5333.9286\n",
      "Epoch 80/100\n",
      "42109/42109 [==============================] - 23s 547us/sample - loss: 2088.4206 - val_loss: 5199.4240\n",
      "Epoch 81/100\n",
      "42109/42109 [==============================] - 23s 544us/sample - loss: 1998.7695 - val_loss: 5397.5041\n",
      "Epoch 82/100\n",
      "42109/42109 [==============================] - 23s 547us/sample - loss: 1855.0975 - val_loss: 5314.5297\n",
      "Epoch 83/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 1863.6739 - val_loss: 5217.5672\n",
      "Epoch 84/100\n",
      "42109/42109 [==============================] - 23s 548us/sample - loss: 1766.8528 - val_loss: 5281.1299\n",
      "Epoch 85/100\n",
      "42109/42109 [==============================] - 23s 544us/sample - loss: 1774.1297 - val_loss: 5277.9814\n",
      "Epoch 86/100\n",
      "42109/42109 [==============================] - 23s 543us/sample - loss: 1735.5268 - val_loss: 5210.5315\n",
      "Epoch 87/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 1650.3491 - val_loss: 5168.0287\n",
      "Epoch 88/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 1603.8940 - val_loss: 5538.8330\n",
      "Epoch 89/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 1624.0825 - val_loss: 5272.2862\n",
      "Epoch 90/100\n",
      "42109/42109 [==============================] - 23s 548us/sample - loss: 1565.5671 - val_loss: 5413.5186\n",
      "Epoch 91/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 1609.2490 - val_loss: 5425.3972\n",
      "Epoch 92/100\n",
      "42109/42109 [==============================] - 23s 543us/sample - loss: 1564.7002 - val_loss: 5382.0158\n",
      "Epoch 93/100\n",
      "42109/42109 [==============================] - 23s 544us/sample - loss: 1427.5908 - val_loss: 5604.0500\n",
      "Epoch 94/100\n",
      "42109/42109 [==============================] - 23s 547us/sample - loss: 1458.8878 - val_loss: 5420.2929\n",
      "Epoch 95/100\n",
      "42109/42109 [==============================] - 23s 545us/sample - loss: 1350.1388 - val_loss: 5430.7407\n",
      "Epoch 96/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 1372.1715 - val_loss: 5613.9612\n",
      "Epoch 97/100\n",
      "42109/42109 [==============================] - 23s 549us/sample - loss: 1379.2925 - val_loss: 5529.5972\n",
      "Epoch 98/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 1399.0442 - val_loss: 5716.3772\n",
      "Epoch 99/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 1353.9709 - val_loss: 5578.1778\n",
      "Epoch 100/100\n",
      "42109/42109 [==============================] - 23s 546us/sample - loss: 1257.4220 - val_loss: 5446.2696\n",
      "complete without error\n"
     ]
    }
   ],
   "source": [
    "##setting up callback for tensor board\n",
    "# Load the TensorBoard notebook extension\n",
    "\n",
    "\n",
    "##saving checkpoint\n",
    "model_checkpoint_callback1 = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='./saved_model/checkpoint/max',\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    "    \n",
    ")\n",
    "model_checkpoint_callback2 = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='./saved_model/checkpoint/e50',\n",
    "    save_weights_only=False,\n",
    "    save_freq=50\n",
    ")\n",
    "##setting up model\n",
    "\n",
    "# for CNN-LSTM reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "# if not using CNN, just comment this line\n",
    "#Xa=Xa.reshape(Xa.shape[0],Xa.shape[1],Xa.shape[2],1)\n",
    "\n",
    "#model\n",
    "model = Sequential()\n",
    "#model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, 9,1)))\n",
    "#model.add(TimeDistributed(MaxPooling1D(pool_size=9)))\n",
    "#model.add(TimeDistributed(Flatten()))\n",
    "model.add(Bidirectional(LSTM(500, activation='tanh',return_sequences=True, recurrent_activation='sigmoid', use_bias=True , input_shape=(18,9))))\n",
    "model.add(Bidirectional(LSTM(200, activation='tanh',return_sequences=True, recurrent_activation='sigmoid', use_bias=True )))\n",
    "model.add(Bidirectional(LSTM(50, activation='tanh',return_sequences=True, recurrent_activation='sigmoid', use_bias=True )))\n",
    "model.add(Bidirectional(LSTM(50, activation='relu',  recurrent_activation='sigmoid', use_bias=True )))\n",
    "model.add(Dense(2))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "#fitting the model\n",
    "#fitting the model\n",
    "model.fit(x=Xa,\n",
    "          y=Ya,\n",
    "          batch_size=1000,\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          callbacks=[\n",
    "              #tensorboard_callback,\n",
    "              model_checkpoint_callback1,model_checkpoint_callback2 ],\n",
    "          validation_data=(Xatest,Yatest)\n",
    "         )\n",
    "\n",
    "model.save('./saved_model/angleModel')\n",
    "\n",
    "#notice that this cell has no error\n",
    "print('complete without error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training done\n",
      "1/1 [==============================] - 1s 844ms/sample\n",
      "[[368.50858  63.78729]]\n"
     ]
    }
   ],
   "source": [
    "print('training done')\n",
    "#demonstrate prediction\n",
    "x_input = Xa[0,:,:]\n",
    "x_input=x_input.reshape((1,18,9))\n",
    "yhat = model.predict(x_input,verbose=1)\n",
    "print(yhat)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
