{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "##Author: Simona\n",
    "#References\n",
    "#https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "#https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/\n",
    "#https://towardsdatascience.com/media/935b97e7b4c541849529cf3b40e4e5ac\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from numpy import load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n",
    "\n",
    "#List of all x and y files in the database\n",
    "path = os.listdir(\"../../MachineLearning SourceCode/Dataset-vectorized/itasserSx9\")\n",
    "x_List = []\n",
    "y_List = []\n",
    "xyfileTuple = []\n",
    "\n",
    "for names in path[:200]:                  ####choose here the files to process\n",
    "    if names.endswith(\"-x-.npy\"):\n",
    "        x_List.append(names)\n",
    "    else:\n",
    "        y_List.append(names)\n",
    "\n",
    "for xfile in x_List:\n",
    "    for yfile in y_List:\n",
    "        if (xfile.replace('-x-.npy','') == yfile.replace('-y-.npy','')):\n",
    "            xyfileTuple.append((xfile,yfile))\n",
    "            break\n",
    "            \n",
    "## Define dataset x_data, y_data\n",
    "##lists of N_batches elements of shape (N_sequence, features)\n",
    "x_data = []           # x as list of inputs array from different files\n",
    "y_data = []           # y as list of outputs array from different files\n",
    "\n",
    "N_batches = len(x_List)\n",
    "for i in range(N_batches):\n",
    "    x_i, y_i = load(\"../../MachineLearning SourceCode/Dataset-vectorized/itasserSx9\"+xyfileTuple[i][0]), load(\"../../MachineLearning SourceCode/Dataset-vectorized/itasserSx9\"+xyfileTuple[i][1])\n",
    "    x_i, y_i = torch.from_numpy(x_i).type(torch.FloatTensor), torch.from_numpy(y_i).type(torch.FloatTensor) \n",
    "    x_data.append(x_i)\n",
    "    y_data.append(y_i)\n",
    "    \n",
    "# Padding data to obtain x and y tensors with same amino-sequencelengths (by adding zeros manually) \n",
    "lengths = [len(x) for x in x_data]\n",
    "longest_sequence = max(lengths)\n",
    "batch_size = len(x_data)\n",
    "D_in = x_data[0].shape[1]\n",
    "D_out = y_data[0].shape[1]\n",
    "padded_x = torch.zeros((N_batches, longest_sequence, D_in))    \n",
    "padded_y = torch.zeros((N_batches, longest_sequence, D_out))\n",
    "\n",
    "# copy over the actual sequences\n",
    "for i, x_len in enumerate(lengths):\n",
    "    x_i = x_data[i]\n",
    "    y_i = y_data[i]\n",
    "    padded_x[i, 0:x_len, :] = x_i[:x_len, :]         \n",
    "    padded_y[i, 0:x_len, :] = y_i[:x_len, :]\n",
    "    \n",
    "def makedirs(dirname):\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname) \n",
    "makedirs('./Basic_LSTM_outputs')\n",
    "\n",
    "##x and y are lists of N_batches tensors\n",
    "#each of shape (N_sequence, D_in) and (N_sequence, D_out) \n",
    "\n",
    "#split first 100 data in training and validation\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(padded_x, padded_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Add data normalization to improve the model prediction\n",
    "##Normalize output angles \n",
    "def normalize(x, m, s):\n",
    "    for i in range(len(x)):\n",
    "        x_n = x\n",
    "        x_n[i, : ,0] = (x[i, : ,0]-m[i, 0])/s[i, 0]\n",
    "        x_n[i, : ,1] = (x[i, : ,1]-m[i, 1])/s[i, 1]\n",
    "    return x_n\n",
    "\n",
    "def unnormalize(x_n, m, s):\n",
    "    for i in range(len(x_n)):\n",
    "        x = x_n\n",
    "        x[i, : ,0] = x_n[i, : ,0]*s[i, 0]+m[i, 0]\n",
    "        x[i, : ,1] = x_n[i, : ,1]*s[i, 1]+m[i, 1]\n",
    "    return x\n",
    "\n",
    "mean, std_dev = y_train[:, :, 4:].mean(dim=1), y_train[:, :, 4:].std(dim=1)\n",
    "y_train_norm = normalize(y_train[:, :, 4:], mean, std_dev)\n",
    "y_valid_norm = normalize(y_valid[:, :, 4:], mean, std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set parameters\n",
    "#Data params\n",
    "N_batch_train, N_seq, D_in = x_train.shape \n",
    "N_batch_valid = x_valid.shape[0]\n",
    "D_out = 2 #the last 2 columns contains angles values\n",
    "        \n",
    "# Network params\n",
    "hidden_dim = 50\n",
    "num_layers = 2\n",
    "learning_rate = 1e-3\n",
    "dtype = torch.float\n",
    "    \n",
    "# Build model for classiffication of secondary structures angles\n",
    "# Here we define RNN model as a class\n",
    "class LSTMangles(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_layers, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        #Defining the layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers)      \n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, hidden = self.lstm(x.view(len(x), 1, -1))\n",
    "        output = self.linear(lstm_out.view(len(lstm_out), -1))\n",
    "        return output\n",
    "    \n",
    "model = LSTMangles(D_in, hidden_dim, num_layers, output_size=2)\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "epoch = 400\n",
    "train_loss = torch.zeros((epoch))  \n",
    "valid_loss = torch.zeros((epoch))  \n",
    "best_val_loss = 1000\n",
    "best_t = 0\n",
    "patient = 0\n",
    "t=0\n",
    "\n",
    "# loop to learn the neural network\n",
    "for t in range(epoch):\n",
    "    \n",
    "    sum_loss = 0.0\n",
    "    for i in range(N_batch_train):  \n",
    "        x = x_train[i, :, :]  #parse the x_train values\n",
    "        y = y_train_norm\n",
    "\n",
    "        model.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_function(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        sum_loss+=loss\n",
    "\n",
    "    train_loss[t] = sum_loss.item()/N_batch_train   \n",
    "    \n",
    "    if t % 100 == 0:\n",
    "        print(\"epoch \", t, \"Loss: \", sum_loss.item())\n",
    "    \n",
    "torch.save(model, './Basic_LSTM_outputs/mytraining_angles_norm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non-normalized losses\n",
    "y_pred_train = torch.zeros((N_batch_train, N_seq, 2))\n",
    "with torch.no_grad():\n",
    "    for i in range(N_batch_train):  \n",
    "        x = x_train[i, :, :]  #parse the x_train values\n",
    "        y = y_train_norm  #parse the last 2 columns values of y_train, last 2 contains angleS\n",
    "        y_pred = model(x)\n",
    "        y_pred_train[i, :, :] = y_pred\n",
    "        \n",
    "y_unnorm_train = unnormalize(y_pred_train, mean, std_dev)      \n",
    "loss = loss_function(y_unnorm_train, y_train[:, :, 4:])/N_batch_train\n",
    "print(loss, 'train_loss')\n",
    "\n",
    "y_pred_valid = torch.zeros((N_batch_valid, N_seq, 2))\n",
    "with torch.no_grad():\n",
    "    for i in range(N_batch_valid):\n",
    "        x = x_valid[i, :, :]\n",
    "        y = y_valid_norm\n",
    "        y_pred = model(x)\n",
    "        y_pred_valid[i, :, :] = y_pred\n",
    "        \n",
    "y_unnorm_valid = unnormalize(y_pred_valid, mean, std_dev)      \n",
    "loss = loss_function(y_unnorm_valid, y_valid[:, :, 4:])/N_batch_valid\n",
    "print(loss, 'valid_loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
