{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "##Author: Simona\n",
    "#References\n",
    "#https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "#https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/\n",
    "#https://towardsdatascience.com/media/935b97e7b4c541849529cf3b40e4e5ac\n",
    "#https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from numpy import load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n",
    "\n",
    "#List of all x and y files in the database\n",
    "path = os.listdir(\"../../MachineLearning SourceCode/Dataset-vectorized/itasserSx9\")\n",
    "x_List = []\n",
    "y_List = []\n",
    "xyfileTuple = []\n",
    "\n",
    "for names in path[:200]:                   ####choose here the files to process\n",
    "    if names.endswith(\"-x-.npy\"):\n",
    "        x_List.append(names)\n",
    "    else:\n",
    "        y_List.append(names)\n",
    "\n",
    "for xfile in x_List:\n",
    "    for yfile in y_List:\n",
    "        if (xfile.replace('-x-.npy','') == yfile.replace('-y-.npy','')):\n",
    "            xyfileTuple.append((xfile,yfile))\n",
    "            break\n",
    "            \n",
    "## Define dataset x_data, y_data\n",
    "##lists of N_batches elements of shape (N_sequence, features)\n",
    "x_data = []           # x as list of inputs array from different files\n",
    "y_data = []           # y as list of outputs array from different files\n",
    "\n",
    "N_batches = len(x_List)\n",
    "for i in range(N_batches):\n",
    "    x_i, y_i = load(\"../../MachineLearning SourceCode/Dataset-vectorized/itasserSx9\"+xyfileTuple[i][0]), load(\"../../MachineLearning SourceCode/Dataset-vectorized/itasserSx9\"+xyfileTuple[i][1])\n",
    "    x_i, y_i = torch.from_numpy(x_i).type(torch.FloatTensor), torch.from_numpy(y_i).type(torch.FloatTensor) \n",
    "    x_data.append(x_i)\n",
    "    y_data.append(y_i)\n",
    "    \n",
    "# Padding data to obtain x and y tensors with same amino-sequencelengths (by adding zeros manually) \n",
    "lengths = [len(x) for x in x_data]\n",
    "longest_sequence = max(lengths)\n",
    "batch_size = len(x_data)\n",
    "D_in = x_data[0].shape[1]\n",
    "D_out = y_data[0].shape[1]\n",
    "padded_x = torch.zeros((N_batches, longest_sequence, D_in))    \n",
    "padded_y = torch.zeros((N_batches, longest_sequence, D_out))\n",
    "\n",
    "# copy over the actual sequences\n",
    "for i, x_len in enumerate(lengths):\n",
    "    x_i = x_data[i]\n",
    "    y_i = y_data[i]\n",
    "    padded_x[i, 0:x_len, :] = x_i[:x_len, :]         \n",
    "    padded_y[i, 0:x_len, :] = y_i[:x_len, :]\n",
    "    \n",
    "def makedirs(dirname):\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname) \n",
    "makedirs('./Basic_LSTM_outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##x and y are tensors of shape (N_batch, N_sequence, D_in) and (N_batch,N_sequence, D_out) \n",
    "\n",
    "#split first 100 data in training and validation\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(padded_x[:100, :, :], padded_y[:100, :, :], test_size=0.2)\n",
    "\n",
    "## Set parameters\n",
    "#Data params\n",
    "N_batch_train, N_seq, D_in = x_train.shape \n",
    "N_batch_valid = x_valid.shape[0]\n",
    "D_out = 4 #number of classes, corresponding to the first 4 columns of y_data tensors\n",
    "        #the last 2 columns contains angles values\n",
    "    \n",
    "# Network params\n",
    "hidden_dim = 50\n",
    "num_layers = 4\n",
    "learning_rate = 1e-3\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for classiffication of secondary structures in types a, u, t, b\n",
    "# Here we define LSTM model as a class\n",
    "class LSTMstructures(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_dim, num_layers, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        #Defining the layers\n",
    "        #self.dropout = nn.Dropout(0.3)\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers)      \n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.dropout(x)\n",
    "        lstm_out, hidden = self.lstm(x.view(len(x), 1, -1))\n",
    "        output = self.linear(lstm_out.view(len(lstm_out), -1))\n",
    "        out_scores = F.softmax(output)\n",
    "        return out_scores\n",
    "    \n",
    "\n",
    "model = LSTMstructures(D_in, hidden_dim, num_layers, output_size=4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model.to(device)\n",
    "x_train.to(device)\n",
    "y_train.to(device)\n",
    "x_valid.to(device)\n",
    "y_valid.to(device)\n",
    "train_loss = torch.zeros((1000))  \n",
    "valid_loss = torch.zeros((1000))  \n",
    "best_val_loss = 1000\n",
    "best_t = 0\n",
    "patient = 0\n",
    "t=0\n",
    "\n",
    "# loop to learn the neural network\n",
    "while True:\n",
    "    \n",
    "    sum_loss = 0.0\n",
    "    for i in range(N_batch_train):  \n",
    "        x = x_train[i, :, :].to(device)  #parse the x_train values\n",
    "        y = y_train[i, :, :4].to(device)  #parse the first 4 columns values of y_train for classification\n",
    "\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(x)\n",
    "        loss = criterion(tag_scores, torch.max(y, 1)[1])\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        sum_loss+=loss\n",
    "\n",
    "    train_loss[t] = sum_loss.item()/N_batch_train   \n",
    "    \n",
    "    if t % 100 == 0:\n",
    "        print(\"epoch \", t, \"Loss: \", sum_loss.item())\n",
    "    \n",
    "        \n",
    "    ###calculate each epoch the validation loss\n",
    "    ##use this for parameter tuning and early stopping\n",
    "    sum_valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i in range(N_batch_valid):\n",
    "            x = x_valid[i, :, :]\n",
    "            y = y_valid[i, :, :4] \n",
    "            tag_scores_valid = model(x)\n",
    "            loss = criterion(tag_scores_valid, torch.max(y, 1)[1])\n",
    "            sum_valid_loss += loss\n",
    "    valid_loss[t] = sum_valid_loss.item()/N_batch_valid\n",
    "    \n",
    "    if(sum_valid_loss.item() <= best_val_loss):   #here we impose early stopping\n",
    "        best_val_loss = sum_valid_loss\n",
    "        best_t = t\n",
    "        patient = 0\n",
    "        torch.save(model, './Basic_LSTM_outputs/mytraining_ss.pt')\n",
    "    else:\n",
    "        patient += 1\n",
    "    if(patient>600): break\n",
    "        \n",
    "    t+=1\n",
    "print(best_t)\n",
    "\n",
    "# plot losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.cla()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(np.arange(best_t), train_loss.detach().numpy()[:best_t], 'b-', label='train_loss')\n",
    "plt.plot(np.arange(best_t), valid_loss.detach().numpy()[:best_t], 'g-', label='valid_loss')\n",
    "plt.legend()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(4, 4)\n",
    "plt.savefig('./Basic_LSTM_outputs/loss', dpi=400, bbox_inches='tight')\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of training and validation sets\n",
    "\n",
    "#define metric\n",
    "def binary_accuracy(preds, y):\n",
    "    corrects = (preds.argmax(dim=1) == y.argmax(dim=1))\n",
    "    acc = corrects.sum().float()/float(len(y))\n",
    "    return acc\n",
    "\n",
    "# Testing and accuracy score\n",
    "\n",
    "\n",
    "lenghts = [len(y_data[i]) for i in range(N_batches)]\n",
    "#model = torch.load('./Basic_LSTM_outputs/mytraining_ss.pt')\n",
    "model.eval()\n",
    "\n",
    "train_acc = torch.zeros((N_batch_train))\n",
    "valid_acc = torch.zeros((N_batch_valid))\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for ii in range(N_batch_train):\n",
    "        x = x_train[ii, :, :]\n",
    "        y = y_train[ii, :, :4]\n",
    "        true_lenght = lenghts[ii]\n",
    "        tag_scores = model(x)\n",
    "        \n",
    "        tag_scores = tag_scores[:true_lenght, :]\n",
    "        y = y[:true_lenght, :]\n",
    "        train_acc[ii] = binary_accuracy(tag_scores, y) \n",
    "\n",
    "    \n",
    "    for ii in range(N_batch_valid):\n",
    "        x = x_valid[ii, :, :]\n",
    "        y = y_valid[ii, :, :4]\n",
    "        tag_scores = model(x)\n",
    "        true_lenght = lenghts[N_batch_train+i]\n",
    "        \n",
    "        tag_scores = tag_scores[:true_lenght, :]\n",
    "        y = y[:true_lenght, :]\n",
    "        valid_acc[ii] = binary_accuracy(tag_scores, y) \n",
    "\n",
    "print(torch.mean(train_acc), 'train_acc')\n",
    "print(torch.mean(valid_acc), 'valid_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This in case with early stopping included\n",
    "##Test is performed on the next 20 values\n",
    "\n",
    "x_test, y_test = padded_x[100:, :, :], padded_y[100:, :, :]\n",
    "test_acc = torch.zeros((N_batch_valid))\n",
    "with torch.no_grad():\n",
    "    for ii in range(20):\n",
    "        x = x_test[ii, :, :]\n",
    "        y = y_test[ii, :, :4]\n",
    "        tag_scores = model(x)\n",
    "        true_lenght = lenghts[100+ii]\n",
    "        \n",
    "        tag_scores = tag_scores[:true_lenght, :]\n",
    "        y = y[:true_lenght, :]\n",
    "        test_acc[ii] = binary_accuracy(tag_scores, y) \n",
    "\n",
    "print(torch.mean(valid_acc), 'test_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define RNN model as a class\n",
    "class RNNstructures(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_layers, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, num_layers)      \n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        rnn_out, hidden = self.rnn(x.view(len(x), 1, -1))\n",
    "        output = self.linear(rnn_out.view(len(rnn_out), -1))\n",
    "        out_scores = F.softmax(output)\n",
    "        return out_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define GRU model as a class\n",
    "class GRUstructures(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_dim, output_size, num_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Define the GRU layer\n",
    "        self.gru = nn.GRU(input_size, hidden_dim, num_layers, batch_first=True)\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, hidden = self.gru(x.view(len(x), 1, -1))\n",
    "        output = self.linear(gru_out.view(len(gru_out), -1))\n",
    "        out_scores = F.softmax(output)\n",
    "        return out_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
